{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import urllib2\n",
    "import re\n",
    "import csv\n",
    "from collections import OrderedDict\n",
    "import cookielib\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start = 'http://archiveofourown.org/tags/Sherlock%20(TV)/works'\n",
    "outfile = './ao3_work_sherlock_test2.csv'\n",
    "start_page = 1500\n",
    "max_page = 3870"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cookie_file = './cookie'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_cookie(cookie_file):\n",
    "    cookie = cookielib.MozillaCookieJar(cookie_file)\n",
    "    opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(cookie))\n",
    "    response = opener.open('http://archiveofourown.org/works/5051548?view_adult=true')\n",
    "    cookie.save(ignore_discard=True, ignore_expires=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_cookie(cookie_file):\n",
    "    cookie = cookielib.MozillaCookieJar()\n",
    "    cookie.load(cookie_file, ignore_discard=True, ignore_expires=True)\n",
    "    return cookie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save_cookie(cookie_file)\n",
    "cookie = load_cookie(cookie_file)\n",
    "opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(cookie))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_page(base_url, page_number):\n",
    "    #go to any page number.\n",
    "    return base_url+'?page=' +str(page_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def find_works(page):\n",
    "    #Find all works from a works list page.\n",
    "    works_page = bs(urllib2.urlopen(page))\n",
    "    links = []\n",
    "    for link in works_page.find_all('a'):\n",
    "        try:\n",
    "            url = link.get('href')\n",
    "            url_s = [i for i in url.split('/') if i != '']\n",
    "            if 'work' in url and len(url_s) == 2 and str(url_s[1]).isdigit():\n",
    "                    links.append('http://archiveofourown.org'+link.get('href'))\n",
    "        except:\n",
    "            pass\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def show_full_contents(url):\n",
    "    #go through adult contents filtering.\n",
    "    base = bs(urllib2.urlopen(url))\n",
    "    full_url = url\n",
    "    for link in base.find_all('a'):\n",
    "        if 'Proceed' in link.text:\n",
    "            full_url = url +'?view_adult=true'\n",
    "    return full_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_contents(url, opener=opener):\n",
    "#     get work metadata and contents from the work page.\n",
    "#     print 'Reading url:', url\n",
    "    try:\n",
    "        req = urllib2.Request(url)\n",
    "        page = bs(opener.open(req))\n",
    "        contents = str(page.body.text.encode('utf-8')).replace('\\n','A')\n",
    "    except:\n",
    "#         print 'Unable to read from this url'\n",
    "        contents = ''\n",
    "        pass\n",
    "    return url, contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def write_header(outfile):\n",
    "    f = open(outfile, 'a')\n",
    "    writer = csv.writer(f, delimiter=',')\n",
    "    keys = ['Additional_Tags', 'Archive_Warnings', 'Author', 'Bookmarks', 'Category', 'Chapters', 'Characters',\\\n",
    "             'Comments', 'CompleteDate', 'Fandoms', 'Hits', 'Kudos', 'Language', 'Notes', 'PublishDate', 'Rating',\\\n",
    "             'Relationship', 'Summary', 'Text', 'Title', 'Words']\n",
    "    writer.writerow(keys)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def write_work_content(work_dict,outfile):\n",
    "    #write work metadata and contents as values of a sorted dictionary.\n",
    "    f = open(outfile, 'a')\n",
    "    writer = csv.writer(f, delimiter=',')\n",
    "    try:\n",
    "        writer.writerow(OrderedDict(sorted(work_dict.items())).values())\n",
    "    except:\n",
    "        pass\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#creates dictionary for information in a single work.\n",
    "def create_work_dict(url, contents):\n",
    "#     get work metadata and contents into a dictionary.\n",
    "\n",
    "#     print 'Getting work information from:', url\n",
    "    try:\n",
    "        work = {}\n",
    "\n",
    "        rating = re.findall('Rating:(.*?)<br />',contents) \n",
    "        warning = re.findall('Warnings:(.*?)<br />',contents)\n",
    "        fandom = re.findall('Fandoms:A          AAA(.*?)AAAA|Fandom:A          AAA(.*?)AAAA',contents) \n",
    "        category = re.findall('Category:A          AAA(.*?)AAAA',contents)\n",
    "        relationship = re.findall('Relationships:(.*?)<br />',contents)\n",
    "        characters = re.findall('Characters:(.*?)<br />',contents)\n",
    "        additional = re.findall('Additional Tags:(.*?)<br />',contents) \n",
    "        language = re.findall('Language:A      AA(.*?)A      A',contents)\n",
    "        author = re.findall('A    AA(.*?)AAA',contents)\n",
    "        text = re.findall('Work Text:(.*?)AAAAAAAA|Chapter TextA(.*?)AAAAA|Chapter TextAAAAAA(.*?)',contents)\n",
    "        text = [i for i in text[0] if i != ''] if text != [] else []\n",
    "        title = re.findall('AAAAAAAA      (.*?)A    AA',contents)\n",
    "        summary = re.findall('>Summary: <p>(.*?)</p>',contents)\n",
    "        notes = re.findall('Notes:AA(.*?)AA',contents)\n",
    "        publishdate = re.findall('Published:([0-9]*-[0-9]*-[0-9]*)',contents)\n",
    "        completedate = re.findall('Completed:([0-9]*-[0-9]*-[0-9]*)',contents)\n",
    "        words = re.findall('Words:([0-9]*)',contents)\n",
    "        chapters = re.findall('Chapters:([0-9]*/[0-9]*)',contents)\n",
    "        comments = re.findall('Comments:([0-9]*)',contents)\n",
    "        kudos = re.findall('Kudos:([0-9]*)',contents)\n",
    "        bookmarks = re.findall('Bookmarks:([0-9]*)',contents)\n",
    "        hits = re.findall('Hits:([0-9]*)',contents)       \n",
    "        \n",
    "        work['Rating'] = rating[0] if rating != [] else ''\n",
    "        work['Archive_Warnings'] = warning[0] if warning != [] else ''\n",
    "        work['Fandoms'] = [i for i in fandom[0] if i != ''][0] if fandom != [] else ''\n",
    "        work['Category'] = category[0] if category != [] else ''\n",
    "        work['Relationship'] = relationship[0] if relationship != [] else '' \n",
    "        work['Characters'] = characters[0] if characters != [] else ''\n",
    "        work['Additional_Tags'] = additional[0] if additional != [] else ''\n",
    "        work['Language'] = language[0] if language != [] else ''\n",
    "        work['Author'] = author[0] if author != [] else ''\n",
    "        work['Text']= text[0] if text != [] else ''\n",
    "        work['Title']  = title[0] if title != [] else ''\n",
    "        work['Summary'] = summary[0] if summary != [] else ''\n",
    "        work['Notes'] = notes[0] if notes != [] else ''\n",
    "        work['Comments'] = comments[0] if comments != [] else ''\n",
    "        work['PublishDate'] = publishdate[0] if publishdate != [] else ''\n",
    "        work['CompleteDate'] = completedate[0] if completedate != [] else ''\n",
    "        work['Words'] = words[0] if words != [] else ''\n",
    "        work['Chapters'] = chapters[0] if chapters != [] else ''\n",
    "        work['Comments'] = comments[0] if comments != [] else ''\n",
    "        work['Kudos'] = kudos[0] if kudos != [] else ''\n",
    "        work['Bookmarks'] = bookmarks[0] if bookmarks != [] else ''\n",
    "        work['Hits'] = hits[0] if hits != [] else ''\n",
    "    \n",
    "#     print 'Finished with:', url\n",
    "    except:\n",
    "#         print 'Something went wrong.'\n",
    "        pass\n",
    "    return work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_chapters_list(url,opener=opener):\n",
    "    url_full = show_full_contents(url)\n",
    "    chapters_list = []\n",
    "    navigate = ''\n",
    "    \n",
    "    req = urllib2.Request(url_full)\n",
    "    page = bs(opener.open(req))\n",
    "    for link in page.find_all('a'):\n",
    "        if 'Chapter Index' in link.text and len(link.get('href')) > 1:\n",
    "            navigate = 'http://archiveofourown.org' + link.get('href')\n",
    "    \n",
    "    if navigate != '':\n",
    "        req2 = urllib2.Request(navigate)\n",
    "        page2 = bs(opener.open(req2))\n",
    "        for link in page2.find_all('a'):\n",
    "            if 'chapters' in link.get('href'):\n",
    "                chapters_list.append('http://archiveofourown.org' + link.get('href'))\n",
    "    return chapters_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_single_work(url):\n",
    "    url_full = show_full_contents(url)\n",
    "    u, c = get_contents(url_full)\n",
    "    work = create_work_dict(u, c)\n",
    "    write_work_content(work,outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get_next_chapter('http://archiveofourown.org/works/3078407?view_adult=true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# s = get_contents('http://archiveofourown.org/works/5051548?view_adult=true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# d = create_work_dict('u',str(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# d = create_work_dict('http://archiveofourown.org/works/5205566',str(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# c = bs(urllib2.urlopen('http://archiveofourown.org/works/5051548?view_adult=true'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# req2 = urllib2.Request('http://archiveofourown.org/works/5051548/navigate')\n",
    "# page2 = bs(opener.open(req2))\n",
    "# for link in page2.find_all('a'):\n",
    "#     if 'Chapter' in link.text:\n",
    "#         print link.text, link.get('href')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://archiveofourown.org/works/5144414/chapters/11841533',\n",
       " 'http://archiveofourown.org/works/5144414/chapters/12312005']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ch = get_chapters_list('http://archiveofourown.org/works/5144414/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crawling page: 1500\n",
      "crawling page: 1501\n",
      "crawling page: 1502\n",
      "crawling page: 1505\n",
      "crawling page:"
     ]
    }
   ],
   "source": [
    "#main loop\n",
    "write_header(outfile)\n",
    "start_time = time.clock()\n",
    "count = 0\n",
    "\n",
    "for i in range(start_page, max_page+1):\n",
    "    try:\n",
    "        page = find_page(start, i)\n",
    "        worklist = find_works(page)\n",
    "        for w in worklist:\n",
    "            ch_list = get_chapters_list(w)\n",
    "            if ch_list != []:\n",
    "                read_single_work(w)\n",
    "                for ch in ch_list:\n",
    "                    read_single_work(ch)\n",
    "            else:\n",
    "                read_single_work(w)\n",
    "            count += 1\n",
    "\n",
    "        print 'crawling page:', i\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print 'Saved %s works from %s pages of tag %s in %s seconds .' %(count, i, 'Sherlock Holmes', str(time.clock() - start_time))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
