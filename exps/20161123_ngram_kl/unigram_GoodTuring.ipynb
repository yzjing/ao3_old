{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import re\n",
    "import sys\n",
    "sys.path.append('../../util/')\n",
    "import sgt\n",
    "from collections import Counter\n",
    "from nltk.stem.snowball import EnglishStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "st = EnglishStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a corpus to give to sklearn\n",
    "def create_corpus_for_voc(df):\n",
    "    doc = []\n",
    "    for i in df.Text.tolist():\n",
    "        #Remove some non-ascii characters and 'aa's\n",
    "        i = re.sub(r'aA|aa', 'a', i)\n",
    "        i = re.sub(r'\\\\xe2........|\\\\xc|\\\\xa|\\\\n|[0123456789*_]', '', i)\n",
    "        i = i.lower()\n",
    "        doc.append(i)  \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get a vocabulary using sklearn's filtering\n",
    "def get_voc(corpus, ngram, mindf):\n",
    "    vectorizer = CountVectorizer(stop_words='english', ngram_range=(ngram,ngram),min_df=mindf)\n",
    "    f = vectorizer.fit_transform(corpus)\n",
    "    return set(sorted(vectorizer.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute unigram-frequency dict using the same preprocessing, using only words from the vocabulary\n",
    "def create_unigram_freq_dict(df, voc):\n",
    "    text = []\n",
    "    for line in df.Text.tolist():\n",
    "        line = re.sub(r'aA|aa', 'a', line)\n",
    "        line = re.sub(r'\\\\xe2........|\\\\xc|\\\\xa|\\\\n|[0123456789*_]', '', line).lower()\n",
    "        line = re.findall(u'(?u)\\\\b\\\\w\\\\w+\\\\b', line)\n",
    "        line = [st.stem(word) for word in line if word in voc]\n",
    "        text.append(dict(Counter(line)))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_timelist(df):\n",
    "    timelist = df.PublishDate.drop_duplicates().tolist()\n",
    "    timelist = [str(i)[:7] for i in timelist]\n",
    "    return sorted(list(set(timelist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_df_time(df, time):\n",
    "    return df[df.PublishDate.str[:7] == time]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# calculate unigram probabilities by simple Good Turing smoothing.\n",
    "# imput: unigram-freq dict\n",
    "# output: unigram-prob dict, mimic of a document-term matrix\n",
    "# if unigram is in this doc, prob = the unigram prob calculated by sgt\n",
    "# otherwise, prob = the probability given to \"all unknown unigrams\" by sgt\n",
    "def calc_sgt(line_dict, voc):\n",
    "    prob_line = []\n",
    "    sgt_line = sgt.simpleGoodTuringProbs(line_dict)\n",
    "    num_abs_words = len(voc - set(line_dict.keys()))\n",
    "    for word in voc:\n",
    "        if word in line_dict.keys():\n",
    "            prob_line.append(sgt_line[0][word])\n",
    "        else:\n",
    "            prob_line.append(sgt_line[1]/float(num_abs_words))\n",
    "    return prob_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_kl(p, q):\n",
    "    return sum([p[i]*(np.log2(p[i]/q[i])) for i in range(len(p))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../data/shakespare_william_works_preprocessed.tsv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 172, 2: 38, 3: 22, 4: 6, 5: 6, 6: 2, 7: 2, 20: 1, 24: 1}\n",
      "{1: 125, 2: 34, 3: 15, 4: 2, 5: 2, 6: 2, 8: 1, 10: 1, 11: 1, 20: 1, 24: 1, 29: 1}\n",
      "{1: 153, 2: 63, 3: 26, 4: 6, 5: 9, 6: 4, 7: 4, 8: 3, 9: 1, 10: 1, 12: 1, 13: 1, 14: 1, 15: 1, 19: 2, 20: 1}\n",
      "{1: 88, 2: 12, 3: 3, 5: 1}\n",
      "{1: 209, 2: 94, 3: 47, 4: 29, 5: 20, 6: 14, 7: 19, 8: 7, 9: 4, 10: 4, 11: 5, 12: 1, 13: 2, 14: 2, 15: 2, 17: 1, 18: 2, 19: 1, 21: 1, 22: 1, 23: 2, 24: 1, 25: 2, 26: 1, 29: 1, 35: 1, 36: 1, 43: 1, 136: 1}\n",
      "{1: 139, 2: 38, 3: 15, 4: 7, 5: 3, 6: 3, 7: 2, 16: 1}\n",
      "{1: 47, 2: 11}\n",
      "{1: 215, 2: 84, 3: 44, 4: 39, 5: 10, 6: 11, 7: 13, 8: 6, 9: 6, 10: 4, 11: 3, 12: 4, 14: 2, 15: 2, 84: 1, 22: 1, 106: 1, 62: 1}\n",
      "{1: 116, 2: 34, 3: 5, 4: 4}\n",
      "{1: 165, 2: 46, 3: 17, 4: 7, 5: 4, 7: 2, 12: 1, 20: 1}\n",
      "{1: 126, 2: 33, 3: 17, 4: 12, 5: 3, 6: 4, 7: 1, 8: 1, 10: 1}\n",
      "{1: 154, 2: 56, 3: 18, 4: 12, 5: 3, 6: 2, 7: 2, 8: 4, 9: 1, 10: 1, 12: 1, 13: 1, 14: 1}\n",
      "{1: 87, 2: 16, 3: 9, 4: 6, 5: 1, 8: 1, 9: 1}\n",
      "{1: 88, 2: 16, 3: 14, 4: 6, 5: 2, 6: 4, 7: 3, 9: 1, 10: 1, 14: 1, 17: 1}\n",
      "{1: 92, 2: 18, 3: 8, 4: 6, 5: 2, 6: 1, 8: 1, 12: 1}\n",
      "{1: 126, 2: 24, 3: 8, 4: 4}\n",
      "{1: 170, 2: 46, 3: 16, 4: 6, 5: 2, 6: 2, 7: 3, 8: 3, 10: 1, 11: 1, 26: 1}\n",
      "{1: 191, 2: 53, 3: 21, 4: 6, 5: 3, 6: 2, 7: 4, 15: 1, 17: 1}\n",
      "{1: 127, 2: 26, 3: 19, 4: 4, 5: 4, 6: 4, 7: 3, 9: 1, 10: 4}\n",
      "{1: 183, 2: 42, 3: 19, 4: 10, 5: 1, 6: 2, 7: 1, 8: 1}\n",
      "{1: 63, 2: 6, 3: 5, 4: 1, 6: 1}\n",
      "{1: 361, 2: 168, 3: 83, 4: 38, 5: 29, 6: 22, 7: 20, 8: 11, 9: 4, 10: 4, 11: 7, 12: 2, 14: 2, 60: 1, 18: 1, 19: 1, 22: 1, 23: 1, 24: 1, 28: 1}\n",
      "{1: 209, 2: 64, 3: 20, 4: 9, 5: 8, 6: 5, 8: 2, 9: 1, 13: 1, 16: 1, 21: 1}\n",
      "{1: 114, 2: 12, 4: 1}\n",
      "{1: 103, 2: 24, 3: 8, 4: 4, 5: 3, 6: 2, 7: 1, 9: 1, 11: 1}\n",
      "{1: 302, 2: 88, 3: 54, 4: 28, 5: 15, 6: 5, 7: 4, 8: 2, 11: 1, 12: 2, 13: 2, 14: 1, 15: 1, 17: 1, 53: 1, 81: 1}\n",
      "{1: 189, 2: 70, 3: 16, 4: 5, 5: 6, 6: 6, 9: 2, 10: 1, 17: 1, 19: 2}\n",
      "{1: 380, 2: 143, 3: 72, 4: 37, 5: 30, 6: 11, 7: 13, 8: 5, 9: 3, 10: 6, 11: 3, 12: 3, 13: 2, 14: 1, 15: 1, 16: 1, 17: 1, 20: 1, 23: 2, 24: 1, 25: 1, 26: 1, 29: 1, 30: 1, 31: 1, 36: 1, 39: 1}\n",
      "{1: 124, 2: 26, 3: 19, 4: 5, 5: 4, 6: 1, 7: 3, 8: 1, 14: 1}\n",
      "{1: 149, 2: 13, 3: 7, 4: 3}\n",
      "{1: 214, 2: 54, 3: 15, 4: 15, 5: 3, 6: 4, 7: 3, 8: 6, 9: 1, 11: 1, 17: 1, 25: 1}\n",
      "{1: 359, 2: 134, 3: 70, 4: 38, 5: 27, 6: 16, 7: 13, 8: 9, 9: 4, 10: 4, 11: 9, 12: 1, 13: 1, 15: 3, 21: 1, 22: 1, 23: 1, 24: 1, 26: 1, 29: 1, 31: 1, 44: 1}\n",
      "{1: 99, 2: 18, 3: 6, 4: 4}\n",
      "{1: 18, 2: 4, 3: 1}\n",
      "{1: 22, 2: 1, 3: 1}\n",
      "{1: 192, 2: 75, 3: 24, 4: 17, 5: 10, 6: 5, 7: 5, 8: 5, 9: 2, 11: 3, 12: 2, 13: 1, 34: 1, 17: 1, 18: 1, 30: 1, 37: 2}\n",
      "{32: 1, 1: 177, 2: 74, 3: 38, 4: 26, 5: 12, 6: 9, 7: 11, 8: 3, 9: 2, 10: 4, 11: 2, 12: 2, 13: 1, 15: 1, 18: 1}\n",
      "{1: 206, 2: 90, 3: 34, 4: 17, 5: 13, 6: 6, 7: 6, 8: 2, 11: 1, 14: 1, 15: 1, 16: 1, 50: 1, 22: 1}\n",
      "{1: 149, 2: 52, 3: 18, 4: 10, 5: 5, 8: 1, 9: 1}\n",
      "{1: 52, 2: 6, 3: 3}\n",
      "{1: 147, 2: 36, 3: 25, 4: 4, 5: 3, 6: 1, 7: 1, 8: 1, 16: 1}\n",
      "{1: 167, 2: 50, 3: 13, 4: 9, 5: 2, 6: 4, 7: 2, 10: 1, 11: 1, 14: 1, 19: 1}\n",
      "{1: 62, 2: 10, 3: 5, 4: 2}\n",
      "{1: 175, 2: 65, 3: 37, 4: 26, 5: 12, 6: 7, 7: 1, 8: 7, 9: 6, 10: 4, 11: 1, 12: 4, 13: 1, 14: 2, 15: 1, 16: 1, 17: 1, 38: 1}\n",
      "done in 2.512s.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jingy/anaconda/envs/py27/lib/python2.7/site-packages/scipy/linalg/basic.py:884: RuntimeWarning: internal gelsd driver lwork query error, required iwork dimension not returned. This is likely the result of LAPACK bug 0038, fixed in LAPACK 3.2.2 (released July 21, 2010). Falling back to 'gelss' driver.\n",
      "  warnings.warn(mesg, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "kl_all = []\n",
    "t0 = time()\n",
    "min_df = 2\n",
    "timelist = create_timelist(df)\n",
    "\n",
    "for t in timelist:\n",
    "    sgt_list = []\n",
    "    df_t = create_df_time(df, t)\n",
    "    \n",
    "    # len(df_t) must > min_df\n",
    "    # tune this for filtering?\n",
    "    if len(df_t) > min_df*5:\n",
    "        \n",
    "        # output of the following pipeline:\n",
    "        # a list of lists, each list containing sgt word probablity\n",
    "        # word order is supposed to be the same\n",
    "        corp = create_corpus_for_voc(df_t)\n",
    "        vocab = get_voc(corp,1,min_df)\n",
    "        unigram_dict = create_unigram_freq_dict(df_t, vocab)\n",
    "        for i in unigram_dict:\n",
    "            sgt_list.append(calc_sgt(i, vocab))\n",
    "        \n",
    "        # calculate kl.\n",
    "        # std: \"standard work\", average of the numpy matrix\n",
    "        # calculate kl of each work - std work in each month\n",
    "        # then use the average as kl of the month\n",
    "        sgt_array = np.asarray(sgt_list)\n",
    "        std = np.mean(sgt_array, axis=0)\n",
    "        kl_month = []\n",
    "        for row in sgt_array:\n",
    "            kl = calc_kl(row, std)\n",
    "            kl_month.append(kl)\n",
    "        kl_all.append(np.average([i for i in kl_month if not np.isnan(i)]))\n",
    "\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5419282705453593, 0.69854270178875033, 0.4595797595418778]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kl_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py27]",
   "language": "python",
   "name": "Python [py27]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
