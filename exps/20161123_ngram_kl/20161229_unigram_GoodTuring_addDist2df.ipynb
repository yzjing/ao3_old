{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import re\n",
    "import sys\n",
    "sys.path.append('../../util/')\n",
    "import sgt\n",
    "from collections import Counter\n",
    "from nltk.stem.snowball import EnglishStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "st = EnglishStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a corpus to give to sklearn\n",
    "def create_corpus_for_voc(df):\n",
    "    doc = []\n",
    "    for i in df.Text.tolist():\n",
    "        #Remove some non-ascii characters and 'aa's\n",
    "        i = re.sub(r'aA|aa', 'a', i)\n",
    "        i = re.sub(r'\\\\xe2........|\\\\xc|\\\\xa|\\\\n|[0123456789*_]', '', i)\n",
    "        i = i.lower()\n",
    "        doc.append(i)  \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get a vocabulary using sklearn's filtering\n",
    "def get_voc(corpus, ngram, mindf):\n",
    "    vectorizer = CountVectorizer(stop_words='english', ngram_range=(ngram,ngram),min_df=mindf)\n",
    "    f = vectorizer.fit_transform(corpus)\n",
    "    return set(sorted(vectorizer.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compute unigram-frequency dict using the same preprocessing, using only words from the vocabulary\n",
    "def create_unigram_freq_dict(df, voc):\n",
    "    text = {}\n",
    "    df_text = df.Text.tolist()\n",
    "    for line in df_text:\n",
    "        line_f = re.sub(r'aA|aa', 'a', line)\n",
    "        line_f = re.sub(r'\\\\xe2........|\\\\xc|\\\\xa|\\\\n|[0123456789*_]', '', line_f).lower()\n",
    "        line_f = re.findall(u'(?u)\\\\b\\\\w\\\\w+\\\\b', line_f)\n",
    "        line_f = [st.stem(word) for word in line_f if word in voc]\n",
    "        text[line] = dict(Counter(line_f))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_timelist(df):\n",
    "    timelist = df.PublishDate.drop_duplicates().tolist()\n",
    "    timelist = [str(i)[:7] for i in timelist]\n",
    "    return sorted(list(set(timelist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_df_time(df, time):\n",
    "    return df[df.PublishDate.str[:7] == time]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# calculate unigram probabilities by simple Good Turing smoothing.\n",
    "# imput: unigram-freq dict\n",
    "# output: unigram-prob dict, mimic of a document-term matrix\n",
    "# if unigram is in this doc, prob = the unigram prob calculated by sgt\n",
    "# otherwise, prob = the probability given to \"all unknown unigrams\" by sgt\n",
    "def calc_sgt(line_dict, voc):\n",
    "    prob_line = []\n",
    "    sgt_line = sgt.simpleGoodTuringProbs(line_dict)\n",
    "    num_abs_words = len(voc - set(line_dict.keys()))\n",
    "    for word in voc:\n",
    "        if word in line_dict.keys():\n",
    "            prob_line.append(sgt_line[0][word])\n",
    "        else:\n",
    "            prob_line.append(sgt_line[1]/float(num_abs_words))\n",
    "    return prob_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_kl(p, q):\n",
    "    return sum([p[i]*(np.log2(p[i]/q[i])) for i in range(len(p))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_dist(row):\n",
    "    return calc_sgt(unigram_dict[row['Text']], vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: slope is > -1.0\n",
      "Warning: reached unobserved count before crossing the smoothing threshold.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jingy/anaconda/envs/py27/lib/python2.7/site-packages/scipy/linalg/basic.py:884: RuntimeWarning: internal gelsd driver lwork query error, required iwork dimension not returned. This is likely the result of LAPACK bug 0038, fixed in LAPACK 3.2.2 (released July 21, 2010). Falling back to 'gelss' driver.\n",
      "  warnings.warn(mesg, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "# tune this for filtering?\n",
    "\n",
    "min_df = 2\n",
    "if len(df) > min_df*10:\n",
    "    \n",
    "    # Add a column of smoothed unigram probablities to df\n",
    "    corp = create_corpus_for_voc(df)\n",
    "    vocab = get_voc(corp,1,min_df)\n",
    "    unigram_dict = create_unigram_freq_dict(df, vocab)\n",
    "df['Dist'] = df.apply(lambda row: get_dist(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Take average of distributions of the month\n",
    "def calc_monthly_std(df, month):\n",
    "    df_t = create_df_time(df, month)\n",
    "    sgt_array = np.asarray(df_t.Dist.tolist())\n",
    "    std = np.mean(sgt_array, axis=0)\n",
    "    return std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# kl between a distribution and std of the month\n",
    "def calc_kl2std(dist, std_month):\n",
    "    try:\n",
    "        return calc_kl(dist, std_month)\n",
    "    except:\n",
    "        return float('nan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make a dict of std to reduce calculation\n",
    "timelist = create_timelist(df)\n",
    "std_all = {}\n",
    "for time in timelist:\n",
    "    std_all[time] = calc_monthly_std(df_t, time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['KL'] = df.apply(lambda row: calc_kl2std(row['Dist'], std_all.get(str(row['PublishDate'])[:7])), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('../../data/unigram_kl/shakespare_william_works_processed3.tsv', sep = '\\t',error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      NaN\n",
       "1      NaN\n",
       "2      NaN\n",
       "3      NaN\n",
       "4      NaN\n",
       "5      NaN\n",
       "6      NaN\n",
       "7      NaN\n",
       "8      NaN\n",
       "9      NaN\n",
       "10     NaN\n",
       "11     NaN\n",
       "12     NaN\n",
       "13     NaN\n",
       "14     NaN\n",
       "15     NaN\n",
       "16     NaN\n",
       "17     NaN\n",
       "18     NaN\n",
       "19     NaN\n",
       "20     NaN\n",
       "21     NaN\n",
       "22     NaN\n",
       "23     NaN\n",
       "24     NaN\n",
       "25     NaN\n",
       "26     NaN\n",
       "27     NaN\n",
       "28     NaN\n",
       "29     NaN\n",
       "        ..\n",
       "3556   NaN\n",
       "3557   NaN\n",
       "3558   NaN\n",
       "3559   NaN\n",
       "3560   NaN\n",
       "3561   NaN\n",
       "3562   NaN\n",
       "3563   NaN\n",
       "3564   NaN\n",
       "3565   NaN\n",
       "3566   NaN\n",
       "3567   NaN\n",
       "3568   NaN\n",
       "3569   NaN\n",
       "3570   NaN\n",
       "3571   NaN\n",
       "3572   NaN\n",
       "3573   NaN\n",
       "3574   NaN\n",
       "3575   NaN\n",
       "3576   NaN\n",
       "3577   NaN\n",
       "3578   NaN\n",
       "3579   NaN\n",
       "3580   NaN\n",
       "3581   NaN\n",
       "3582   NaN\n",
       "3583   NaN\n",
       "3584   NaN\n",
       "3585   NaN\n",
       "Name: KL, dtype: float64"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.KL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py27]",
   "language": "python",
   "name": "Python [py27]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
