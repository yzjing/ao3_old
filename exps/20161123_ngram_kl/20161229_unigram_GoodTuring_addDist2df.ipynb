{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import re\n",
    "import sys\n",
    "sys.path.append('../../util/')\n",
    "import sgt\n",
    "from collections import Counter\n",
    "from nltk.stem.snowball import EnglishStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "st = EnglishStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a corpus to give to sklearn\n",
    "def create_corpus_for_voc(df):\n",
    "    doc = []\n",
    "    for i in df.Text.tolist():\n",
    "        #Remove some non-ascii characters and 'aa's\n",
    "        i = re.sub(r'aA|aa', 'a', i)\n",
    "        i = re.sub(r'\\\\xe2........|\\\\xc|\\\\xa|\\\\n|[0123456789*_]', '', i)\n",
    "        i = i.lower()\n",
    "        doc.append(i)  \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get a vocabulary using sklearn's filtering\n",
    "def get_voc(corpus, ngram, mindf):\n",
    "    vectorizer = CountVectorizer(stop_words='english', ngram_range=(ngram,ngram),min_df=mindf)\n",
    "    f = vectorizer.fit_transform(corpus)\n",
    "    return set(sorted(vectorizer.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compute unigram-frequency dict using the same preprocessing, using only words from the vocabulary\n",
    "def create_unigram_freq_dict(df, voc):\n",
    "    text = {}\n",
    "    df_text = df.Text.tolist()\n",
    "    for line in df_text:\n",
    "        line_f = re.sub(r'aA|aa', 'a', line)\n",
    "        line_f = re.sub(r'\\\\xe2........|\\\\xc|\\\\xa|\\\\n|[0123456789*_]', '', line_f).lower()\n",
    "        line_f = re.findall(u'(?u)\\\\b\\\\w\\\\w+\\\\b', line_f)\n",
    "        line_f = [st.stem(word) for word in line_f if word in voc]\n",
    "        text[line] = dict(Counter(line_f))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_timelist(df):\n",
    "    timelist = df.PublishDate.drop_duplicates().tolist()\n",
    "    timelist = [str(i)[:7] for i in timelist]\n",
    "    return sorted(list(set(timelist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_df_time(df, time):\n",
    "    return df[df.PublishDate.str[:7] == time]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# calculate unigram probabilities by simple Good Turing smoothing.\n",
    "# imput: unigram-freq dict\n",
    "# output: unigram-prob dict, mimic of a document-term matrix\n",
    "# if unigram is in this doc, prob = the unigram prob calculated by sgt\n",
    "# otherwise, prob = the probability given to \"all unknown unigrams\" by sgt\n",
    "def calc_sgt(line_dict, voc):\n",
    "    prob_line = []\n",
    "    sgt_line = sgt.simpleGoodTuringProbs(line_dict)\n",
    "    num_abs_words = len(voc - set(line_dict.keys()))\n",
    "    for word in voc:\n",
    "        if word in line_dict.keys():\n",
    "            prob_line.append(sgt_line[0][word])\n",
    "        else:\n",
    "            prob_line.append(sgt_line[1]/float(num_abs_words))\n",
    "    return prob_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_kl(p, q):\n",
    "    return sum([p[i]*(np.log2(p[i]/q[i])) for i in range(len(p))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_dist(row):\n",
    "    return calc_sgt(unigram_dict[row['Text']], vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: slope is > -1.0\n",
      "Warning: reached unobserved count before crossing the smoothing threshold.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jingy/anaconda/envs/py27/lib/python2.7/site-packages/scipy/linalg/basic.py:884: RuntimeWarning: internal gelsd driver lwork query error, required iwork dimension not returned. This is likely the result of LAPACK bug 0038, fixed in LAPACK 3.2.2 (released July 21, 2010). Falling back to 'gelss' driver.\n",
      "  warnings.warn(mesg, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "# tune this for filtering?\n",
    "\n",
    "min_df = 2\n",
    "if len(df) > min_df*10:\n",
    "    \n",
    "    # Add a column of smoothed unigram probablities to df\n",
    "    corp = create_corpus_for_voc(df)\n",
    "    vocab = get_voc(corp,1,min_df)\n",
    "    unigram_dict = create_unigram_freq_dict(df, vocab)\n",
    "df['Dist'] = df.apply(lambda row: get_dist(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Take average of distributions of the month\n",
    "def calc_monthly_std(df, month):\n",
    "    df_t = create_df_time(df, month)\n",
    "    sgt_array = np.asarray(df_t.Dist.tolist())\n",
    "    std = np.mean(sgt_array, axis=0)\n",
    "    return std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# kl between a distribution and std of the month\n",
    "def calc_kl2std(dist, std_month):\n",
    "    try:\n",
    "        return calc_kl(dist, std_month)\n",
    "    except:\n",
    "        return float('nan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make a dict of std to reduce calculation\n",
    "timelist = create_timelist(df)\n",
    "std_all = {}\n",
    "for time in timelist:\n",
    "    std_all[time] = calc_monthly_std(df_t, time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['KL'] = df.apply(lambda row: calc_kl2std(row['Dist'], std_all.get(str(row['PublishDate'])[:7])), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py27]",
   "language": "python",
   "name": "Python [py27]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
