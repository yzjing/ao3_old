{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import urllib2\n",
    "import re\n",
    "import csv\n",
    "from collections import OrderedDict, Counter\n",
    "import cookielib\n",
    "import time\n",
    "import random\n",
    "\n",
    "start = 'http://archiveofourown.org/tags/Sherlock%20(TV)/works'\n",
    "\n",
    "outfile = './sherlock_1_100.csv'\n",
    "logfile = './log_1_100'\n",
    "max_page = 100\n",
    "try:\n",
    "    start_page = int([line.strip() for line in open(logfile, 'r')][-1])\n",
    "except:\n",
    "    start_page = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cookie_file = './cookie'\n",
    "\n",
    "def save_cookie(cookie_file):\n",
    "    cookie = cookielib.MozillaCookieJar(cookie_file)\n",
    "    opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(cookie))\n",
    "    response = opener.open('http://archiveofourown.org/works/5051548?view_adult=true')\n",
    "    cookie.save(ignore_discard=True, ignore_expires=True)\n",
    "\n",
    "def load_cookie(cookie_file):\n",
    "    cookie = cookielib.MozillaCookieJar()\n",
    "    cookie.load(cookie_file, ignore_discard=True, ignore_expires=True)\n",
    "    return cookie\n",
    "\n",
    "# save_cookie(cookie_file)\n",
    "cookie = load_cookie(cookie_file)\n",
    "opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(cookie))\n",
    "\n",
    "def find_page(base_url, page_number):\n",
    "    #go to any page number.\n",
    "    return base_url+'?page=' +str(page_number)\n",
    "\n",
    "def find_works(page):\n",
    "    #Find all works from a works list page.\n",
    "    links = []\n",
    "    try:\n",
    "        works_page = bs(urllib2.urlopen(page))\n",
    "        for link in works_page.find_all('a'):\n",
    "            url = link.get('href')\n",
    "            url_s = [i for i in url.split('/') if i != '']\n",
    "            if 'work' in url and len(url_s) == 2 and str(url_s[1]).isdigit():\n",
    "                    links.append('http://archiveofourown.org'+link.get('href'))\n",
    "    except:\n",
    "        pass\n",
    "    return links\n",
    "\n",
    "def show_full_contents(url):\n",
    "    #go through adult contents filtering.\n",
    "    full_url = url\n",
    "    try:\n",
    "        base = bs(urllib2.urlopen(url))\n",
    "        for link in base.find_all('a'):\n",
    "            if 'Proceed' in link.text:\n",
    "                full_url = url +'?view_adult=true'\n",
    "    except:\n",
    "        pass\n",
    "    return full_url\n",
    "\n",
    "def get_contents(url, opener=opener):\n",
    "#     get work metadata and contents from the work page.\n",
    "    try:\n",
    "        req = urllib2.Request(url)\n",
    "        page = bs(opener.open(req))\n",
    "        contents = str(page.body.text.encode('utf-8'))\n",
    "    except:\n",
    "        contents = ''\n",
    "    return url, contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('http://archiveofourown.org/works/6593737',\n",
       " '\\n\\nMain Content\\n\\n\\n\\n    While we\\'ve done our best to make the core functionality of this site accessible without javascript, it will work better with it enabled. Please consider turning it on!\\n  \\n\\n\\n\\n\\n\\nArchive of Our Own beta\\n\\n\\n\\nLog In\\n\\n\\n\\n\\nUser name:\\n\\nPassword:\\n\\n\\n\\nRemember Me\\n\\n\\n\\n\\nForgot password?\\n\\nGet an Invitation\\n\\n\\n\\n\\nSite Navigation\\n\\n\\nFandoms\\n\\nAll Fandoms\\nAnime & Manga\\nBooks & Literature\\nCartoons & Comics & Graphic Novels\\nCelebrities & Real People\\nMovies\\nMusic & Bands\\nOther Media\\nTheater\\nTV Shows\\nVideo Games\\nUncategorized Fandoms\\n\\n\\n\\nBrowse\\n\\nWorks\\nBookmarks\\nTags\\nCollections\\n\\n\\n\\nSearch\\n\\nWorks\\nBookmarks\\nTags\\nPeople\\n\\n\\n\\nAbout\\n\\nAbout Us\\nNews\\nFAQ\\nWrangling Guidelines\\nDonate or Volunteer\\n\\n\\n\\n\\nSearch Works\\n\\nWork Search:\\n\\ntip: lex m/m (mature OR explicit)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\xc2\\xa0\\nSkip header\\n\\n\\n\\n\\n\\n\\n\\n\\nActions\\n\\n\\nComments \\n\\n\\nShare\\n\\nCopy and paste the following code to link back to this work (CTRL A/CMD A will select all), or use the Tweet or Tumblr links to share the work on your Twitter or Tumblr account.\\n\\n<a href=\"http://archiveofourown.org/works/6593737\"><strong>In A Million Different Universes...</strong></a> (1237 words) by <a href=\"http://archiveofourown.org/users/afteriwake\"><strong>afteriwake</strong></a><br />Chapters: 1/1<br />Fandom: <a href=\"http://archiveofourown.org/tags/Sherlock%20(TV)\">Sherlock (TV)</a>, <a href=\"http://archiveofourown.org/tags/Doctor%20Who\">Doctor Who</a>, <a href=\"http://archiveofourown.org/tags/Bleach\">Bleach</a>, <a href=\"http://archiveofourown.org/tags/Cowboy%20Bebop\">Cowboy Bebop</a>, <a href=\"http://archiveofourown.org/tags/Eureka\">Eureka</a>, <a href=\"http://archiveofourown.org/tags/CSI:%20NY\">CSI: NY</a>, <a href=\"http://archiveofourown.org/tags/Stargate%20Atlantis\">Stargate Atlantis</a>, <a href=\"http://archiveofourown.org/tags/Supernatural\">Supernatural</a>, <a href=\"http://archiveofourown.org/tags/Buffy%20the%20Vampire%20Slayer\">Buffy the Vampire Slayer</a><br />Rating: General Audiences<br />Warnings: No Archive Warnings Apply<br />Relationships: Sherlock Holmes/Amy Pond (Doctor Who), Hitsugaya Toshiro/Edward Wong Hau Pepelu Tivrusky IV, Eleventh Doctor/Tess Fontana, Zane Donovan/Amy Pond, Danny Messer/Teyla Emmagan, Greg Lestrade & Amy Pond, Buffy Summers/Dean Winchester<br />Characters: Sherlock Holmes, Amy Pond, The Doctor (Doctor Who), Eleventh Doctor, Faye Valentine, Hitsugaya Toushirou, Spike Spiegel, Kurosaki Ichigo, Edward Wong Hau Pepelu Tivrusky IV, Tess Fontana, Danny Messer, Teyla Emmagan, Greg Lestrade, Buffy Summers, Dean Winchester<br />Additional Tags: Drabble, Drabble Collection, Crossover, Crossover Pairings, Multiple Crossovers, Crack Crossover, Humor, Alien Invasion, Sparring, Kissing, Running Away<br />Summary: <p>There is a theory that the universe makes copies of itself to account for all the possibilities and these duplicates will proceed independently, so in one universe things go one way and in one go another, and so in one universe these characters may have met and in another a different set of characters may have met. These are some crossover meetings that <i>could</i> have happened, somehow, somewhere.</p>\\n\\n\\n\\n\\n        Tweet\\n      \\n\\n\\n\\n        Share on Tumblr\\n      \\n\\n\\n\\n\\n\\nDownload\\n\\nMOBI\\nEPUB\\nPDF\\nHTML\\n\\n\\n\\n\\nWork Header\\n\\n\\n\\n              Rating:\\n          \\n\\n\\nGeneral Audiences\\n\\n\\n\\nArchive Warning:\\n          \\n\\n\\nNo Archive Warnings Apply\\n\\n\\n\\n              Categories:\\n          \\n\\n\\nF/MGen\\n\\n\\n\\n              Fandoms:\\n          \\n\\n\\nSherlock (TV)Doctor WhoBleachCowboy BebopEurekaCSI: NYStargate AtlantisSupernaturalBuffy the Vampire Slayer\\n\\n\\n\\n              Relationships:\\n          \\n\\n\\nSherlock Holmes/Amy Pond (Doctor Who)Hitsugaya Toshiro/Edward Wong Hau Pepelu Tivrusky IVEleventh Doctor/Tess FontanaZane Donovan/Amy PondDanny Messer/Teyla EmmaganGreg Lestrade & Amy PondBuffy Summers/Dean Winchester\\n\\n\\n\\n              Characters:\\n          \\n\\n\\nSherlock HolmesAmy PondThe Doctor (Doctor Who)Eleventh DoctorFaye ValentineHitsugaya ToushirouSpike SpiegelKurosaki IchigoEdward Wong Hau Pepelu Tivrusky IVTess FontanaDanny MesserTeyla EmmaganGreg LestradeBuffy SummersDean Winchester\\n\\n\\n\\n              Additional Tags:\\n          \\n\\n\\nDrabbleDrabble CollectionCrossoverCrossover PairingsMultiple CrossoversCrack CrossoverHumorAlien InvasionSparringKissingRunning Away\\n\\n\\n\\n        Language:\\n      \\n\\n        English\\n      \\nStats:\\n\\n\\nPublished:2016-04-18Words:1237Chapters:1/1Comments:1Kudos:1Hits:27\\n\\n\\n\\n\\n\\n\\n\\n      In A Million Different Universes...\\n    \\n\\nafteriwake\\n\\n\\nSummary:\\n\\nThere is a theory that the universe makes copies of itself to account for all the possibilities and these duplicates will proceed independently, so in one universe things go one way and in one go another, and so in one universe these characters may have met and in another a different set of characters may have met. These are some crossover meetings that could have happened, somehow, somewhere.\\n\\n\\n\\nNotes:\\n\\nWhile I was moving my files from one computer to another I ran across a ton of crossover drabbles I had written and I finally got them cleaned up and organized. The first Wholock one was written for LadyEmmalineWrites1812 who had wanted me to write a Pondlock drabble with the prompt \"running.\"\\n\\n\\n\\n\\n\\n\\nWork Text:\\n\\n\\n\\nLess Flirting, More Running\\nSherlock/Doctor Who\\n\\nSherlock Holmes/Amy Pond & Eleventh Doctor\\n\\n\\n\\xe2\\x80\\x9cI thought you were joking when you said there was an awful lot of running,\\xe2\\x80\\x9d Sherlock said, easily keeping up with both the Doctor and Amy as they tried to outrun the Sontarans chasing after them. He\\xe2\\x80\\x99d been roped into traveling with them on what the Doctor had promised would be a nice, quiet adventure. It had turned into anything but as they got caught in the middle of an all out brawl in a bar that had spilled out in the streets.\\xe2\\x80\\x9cWell, this is only the, what, third time we\\xe2\\x80\\x99ve had to run so far?\\xe2\\x80\\x9d Amy said, grin on her face and twinkle in her eye. \\xe2\\x80\\x9cIf you beat me to the TARDIS I suppose I can reward you with something extra special. Maybe a kiss or\\xe2\\x80\\xa6maybe something more.\\xe2\\x80\\x9dSherlock looked intrigued and was about to reply when the Sontaran closest to them shot at them and the Doctor shoved them both. \\xe2\\x80\\x9cLess flirting, more running!\\xe2\\x80\\x9d he said, shaking his head. He never did understand how his companions could think of their libidos instead of their lives when it came down to the important bits like, oh, he didn\\xe2\\x80\\x99t know, staying alive. Humans. He\\xe2\\x80\\x99d never ever understand them.\\n\\n\\nCute Little Kids\\nBleach/Cowboy Bebop\\n\\nHitsugaya Toshiro/Edward Wong Hau Pepelu Tivrusky IV, Faye Valentine, Spike Spiegal & Kurosaki Ichigo\\n\\n\\n\\xe2\\x80\\x9cYou\\xe2\\x80\\x99re just a little boy!\\xe2\\x80\\x9d Faye said, squealing.Hitsugaya glared at her. \\xe2\\x80\\x9cDo either of you know where we are?\\xe2\\x80\\x9d\\xe2\\x80\\x9cThe Bebop,\\xe2\\x80\\x9d Spike said from the sofa. \\xe2\\x80\\x9cFar in the future, apparently.\\xe2\\x80\\x9dIchigo sighed. \\xe2\\x80\\x9cAny idea how we can get back to the 21st century?\\xe2\\x80\\x9d he asked Faye.\\xe2\\x80\\x9cOnly if the little kid kisses Ed,\\xe2\\x80\\x9d Faye said.\\xe2\\x80\\x9cI do not kiss men,\\xe2\\x80\\x9d Hitsugaya growled.\\xe2\\x80\\x9cEdward will just kiss you then!\\xe2\\x80\\x9d Ed said, hopping from her place in front of the computer, kissing Hitsugaya on the lips.\\xe2\\x80\\x9cStop it!\\xe2\\x80\\x9d he said as he pushed her away, Matsumoto laughing the whole time.\\n\\n\\nSomething Sciency Made Me Do It\\nDoctor Who/Eureka\\n\\nEleventh Doctor/Tess Fontana & Zane Donovan/Amy Pond\\n\\n\\n\\xe2\\x80\\x9cWhere are we, Doctor?\\xe2\\x80\\x9d Amelia asked.\\xe2\\x80\\x9cTrying to fix a mistake in Eureka,\\xe2\\x80\\x9d he replied.\\xe2\\x80\\x9cEureka?\\xe2\\x80\\x9d\\xe2\\x80\\x9cSmall town in the Pacific Northwest, home to a bunch of scientists with too much time and government money on their hands. Ah!\\xe2\\x80\\x9d He tapped a woman on the shoulder. She turned around, grinned, and kissed him. He gently pushed her away. \\xe2\\x80\\x9cTess Fontana?\\xe2\\x80\\x9d\\xe2\\x80\\x9cWha--?\\xe2\\x80\\x9d she said, blinking.\\xe2\\x80\\x9cI\\xe2\\x80\\x99m here to help,\\xe2\\x80\\x9d he said, turning to Amelia. She was too busy being kissed by a guy to say anything.\\xe2\\x80\\x9cZane!\\xe2\\x80\\x9d Tess said. \\xe2\\x80\\x9cStop that!\\xe2\\x80\\x9dThe Doctor grinned. \\xe2\\x80\\x9cLike I said, here to help.\\xe2\\x80\\x9d\\n\\n\\nTo Learn To Fight Properly\\nCSI: NY/Stargate Atlantis\\n\\nDanny Messer/Teyla Emmagan\\n\\n\\nHe looked down at one of Teyla\\'s bantos sticks. \"You fight with these?\" Danny asked.Teyla nodded. \"Yes. It is something I am very good at.\"\"No doubt.\" He nodded towards it. \"Can I?\"\"Please, go ahead,\" Teyla said with a nod. Danny picked up the stick and held it, feeling its weight in his hand. \"If you would like, I can show you how to spar with these.\"\"I think I\\'d like that,\" he said, watching her pick up three of them and accepting one of them when she handed it to him. \"It\\'ll help me take my mind off of things.\"\"That is the purpose of my suggestion,\" she said with a smile. \"Now, if you are ready, I will show you how to prepare to fight with these.\"\\n\\n\\nWanting The Daily Grind Again\\nSherlock/Doctor Who\\n\\nGreg Lestrade & Amy Pond\\n\\n\\n\\xe2\\x80\\x9cI want a gun.\\xe2\\x80\\x9dLestrade looked at Amy, who looked very determined and just a little bit scary. He had no clue about her beyond the fact she was a model and apparently she\\xe2\\x80\\x99d known the Doctor character for a while. He didn\\xe2\\x80\\x99t even know if she could handle a gun. But the determined look, he knew if he didn\\xe2\\x80\\x99t hand her a gun she\\xe2\\x80\\x99d find some other weapon and use it against the alien bastards.Aliens. He\\xe2\\x80\\x99d never believed in them, not even as a child. He\\xe2\\x80\\x99d thought it was all hogwash. And now here he was, nearing fifty when a man in a blue police box dropped into his life, disrupting his vacation. His well-deserved and well-earned vacation, he might add. Aliens were attacking Egypt and it was up to the Doctor, his three friends and him. And if they were right, and this was the end of the world coming? If the woman wanted a gun he\\xe2\\x80\\x99d give her a gun.He handed her a rather large one. She hefted it and then held it like a natural. \\xe2\\x80\\x9cHeld a gun before?\\xe2\\x80\\x9d he asked.\\xe2\\x80\\x9cUsed a machine gun to take down some aliens before,\\xe2\\x80\\x9d she said with a nod. \\xe2\\x80\\x9cNot in this universe, but I remember it.\\xe2\\x80\\x9d\\xe2\\x80\\x9cAh,\\xe2\\x80\\x9d he said with a nod. He didn\\xe2\\x80\\x99t really want to know more about her crazy life. The sooner his life got back to normal, back to the point where the only thing he had to dread was another consult from Sherlock Holmes, the better. \\xe2\\x80\\x9cReady to get to work?\\xe2\\x80\\x9dShe nodded. \\xe2\\x80\\x9cReady as I\\xe2\\x80\\x99ll ever be.\\xe2\\x80\\x9d\\xe2\\x80\\x9cGood. Then let\\xe2\\x80\\x99s take care of the threat and get back to our normal lives.\\xe2\\x80\\x9d He made his way through the tomb entrance with her at his side, and waited. They\\xe2\\x80\\x99d come soon enough, and when they did? He\\xe2\\x80\\x99d send them on their way in a body bag, or however it was you disposed of dead aliens. Then he\\xe2\\x80\\x99d go home and get back to the daily grind he missed so much right now.\\n\\n\\nIn Or Out?\\nSupernatural/Buffy the Vampire Slayer\\n\\nDean Winchester/Buffy Summers\\n\\n\\n\\xe2\\x80\\x9cYou have some options here,\\xe2\\x80\\x9d she said, looking at the situation. \\xe2\\x80\\x9cYou can sit here and let me take care of it, or you can help and make a fool of yourself. Your choice.\\xe2\\x80\\x9d\\xe2\\x80\\x9cSo helping out is optional this round?\\xe2\\x80\\x9d he asked with a slight smirk.\\xe2\\x80\\x9cYeah, this round,\\xe2\\x80\\x9d she said with a nod, gripping the scythe tighter.\\xe2\\x80\\x9cI\\'ll opt out,\\xe2\\x80\\x9d he said, putting his hands up and taking a step back. She grinned at him and went to work on the vampires. These were his type of vampires, not the ones he referred to as \\xe2\\x80\\x9csissy vamps\\xe2\\x80\\x9d that bit it with a stake through the chest. No, these were the full on, cut-their-heads-off vamps that he hated dealing with. Any chance to watch a pro in action, especially that particular pro, was fine by him.After the last head hit the floor she looked at her scythe and ran her finger across the side, flinging the blood off. \\xe2\\x80\\x9cThat was actually kind of fun,\\xe2\\x80\\x9d she said.\\xe2\\x80\\x9cYeah, you have to teach me some of those moves, Buffy,\\xe2\\x80\\x9d he replied.She went over to him and kissed him on the cheek. \\xe2\\x80\\x9cOnly when you get superhuman healing abilities and cat-like reflexes,\\xe2\\x80\\x9d she said with a smile. \\xe2\\x80\\x9cOtherwise I\\'d lose you, and who\\'d I have all the witty banter with?\\xe2\\x80\\x9dHe grinned, and reached over and pulled her into a kiss. \\xe2\\x80\\x9cFaith?\\xe2\\x80\\x9d he murmured onto her lips when they were done.\\xe2\\x80\\x9cOh, that\\'s just a bad joke,\\xe2\\x80\\x9d she said, using her free hand to shove away from him, but she had a smile on her face, so he knew he wasn\\'t on the couch tonight. That was good.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nActions\\n\\n\\xe2\\x86\\x91 Top\\n\\n\\n\\n\\n\\n \\nComments (1)\\n\\n\\nComments\\n\\n\\nScaramou\\n         left kudos on this work!\\n        \\n(collapse)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPost Comment\\n\\nNote:\\nAll fields are required. Your email address will not be published.\\nName: \\n\\n\\nvar validation_for_comment_name_for_6593737=new LiveValidation(\\'comment_name_for_6593737\\',{wait:500,onlyOnBlur:false});validation_for_comment_name_for_6593737.add(Validate.Presence,{\"failureMessage\":\"Please enter your name.\",\"validMessage\":\"\"});\\n\\nEmail: \\n\\n\\nvar validation_for_comment_email_for_6593737=new LiveValidation(\\'comment_email_for_6593737\\',{wait:500,onlyOnBlur:false});validation_for_comment_email_for_6593737.add(Validate.Presence,{\"failureMessage\":\"Please enter your email address.\",\"validMessage\":\"\"});\\n\\n\\n\\nComment\\n\\n\\n\\n\\n4300 characters left\\nvar validation_for_comment_content_for_6593737=new LiveValidation(\\'comment_content_for_6593737\\',{wait:500,onlyOnBlur:false});validation_for_comment_content_for_6593737.add(Validate.Presence,{\"failureMessage\":\"Brevity is the soul of wit, but we need your comment to have text in it.\",\"validMessage\":\"\"});validation_for_comment_content_for_6593737.add(Validate.Length,{\"maximum\":\"4300\",\"tooLongMessage\":\"must be less than 4300 characters long.\"});\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFooter\\n\\n\\nAbout the Archive\\n\\nSite Map\\nDiversity Statement\\nTerms of Service\\nDMCA Policy \\n\\n\\n\\nContact Us\\n\\nReport Abuse\\nTechnical Support and Feedback\\n\\n\\n\\nDevelopment\\n\\notwarchive v0.9.130.2\\nKnown Issues\\nGPL by the OTW\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nif(typeof jQuery==\\'undefined\\'){document.write(unescape(\"%3Cscript src=\\'/javascripts/jquery.min.js\\' type=\\'text/javascript\\'%3E%3C/script%3E\"));document.write(unescape(\"%3Cscript src=\\'/javascripts/jquery-ui.min.js\\' type=\\'text/javascript\\'%3E%3C/script%3E\"));}\\n$j=jQuery.noConflict();\\neval(mod_pagespeed_uWR_ADYg_h);\\neval(mod_pagespeed_pQN2xWGmrj);\\neval(mod_pagespeed_fY6rzhkge2);\\neval(mod_pagespeed_06vpfMn9Bz);\\neval(mod_pagespeed_fnD1p31Tv3);\\neval(mod_pagespeed_AtfUp1elco);\\neval(mod_pagespeed_Y8YLjwkPYa);\\neval(mod_pagespeed_0poykgxpB3);\\neval(mod_pagespeed_ul6QDxmMoL);\\neval(mod_pagespeed_s7RFBAp9AR);\\neval(mod_pagespeed_LaH2WTbyKE);\\n\\nvar name_id=\"#comment_name_for_6593737\";var email_id=\"#comment_email_for_6593737\";if(!$j(name_id).val()){$j(name_id).val($j.cookie(\\'comment_name\\'));}\\nif(!$j(email_id).val()){$j(email_id).val($j.cookie(\\'comment_email\\'));}\\n')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_contents('http://archiveofourown.org/works/6593737')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c = get_contents('http://archiveofourown.org/works/6593737')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' <a href=\"http://archiveofourown.org/tags/Sherlock%20(TV)\">Sherlock (TV)</a>, <a href=\"http://archiveofourown.org/tags/Doctor%20Who\">Doctor Who</a>, <a href=\"http://archiveofourown.org/tags/Bleach\">Bleach</a>, <a href=\"http://archiveofourown.org/tags/Cowboy%20Bebop\">Cowboy Bebop</a>, <a href=\"http://archiveofourown.org/tags/Eureka\">Eureka</a>, <a href=\"http://archiveofourown.org/tags/CSI:%20NY\">CSI: NY</a>, <a href=\"http://archiveofourown.org/tags/Stargate%20Atlantis\">Stargate Atlantis</a>, <a href=\"http://archiveofourown.org/tags/Supernatural\">Supernatural</a>, <a href=\"http://archiveofourown.org/tags/Buffy%20the%20Vampire%20Slayer\">Buffy the Vampire Slayer</a>']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1 = re.findall('Fandom:(.*?)<br />', str(c))\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sherlock (TV)',\n",
       " 'Doctor Who',\n",
       " 'Bleach',\n",
       " 'Cowboy Bebop',\n",
       " 'Eureka',\n",
       " 'CSI: NY',\n",
       " 'Stargate Atlantis',\n",
       " 'Supernatural',\n",
       " 'Buffy the Vampire Slayer']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f2 = re.findall('>(.*?)</a>', f1[0])\n",
    "f2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_comments_time(url, opener=opener):\n",
    "    #go to the comments page of the work and find the timestamps for the comments\n",
    "    #returns a dict of {month:# of comments in the month}\n",
    "    req = urllib2.Request(url)\n",
    "    page = bs(opener.open(req))\n",
    "    times = []\n",
    "    month_dict = {'Jan':'01', 'Feb':'02','Mar':'03', 'Apr':'04', 'May':'05', 'Jun':'06', 'Jul':'07', 'Aug':'08', 'Sep':'09', 'Oct':'10', 'Nov':'11', 'Dec':'12'}\n",
    "    for line in str(page).split('<span class=\"posted datetime\">'):\n",
    "        month = re.findall('Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec', line)[0]\n",
    "        year = re.findall('<span class=\"year\">([0-9]*)</span>', line)\n",
    "        if month != [] and year != []:\n",
    "            times.append(str(year[0]) + '-' + month_dict.get(month))\n",
    "    c = Counter(times)\n",
    "    return {time:c[time] for time in times}\n",
    "\n",
    "def get_bookmarks_time(url, opener=opener):\n",
    "    #go to the bookmarks page of the work and find the timestamps for the bookmarks\n",
    "    #returns a dict of {month:# of bookmarks in the month}\n",
    "\n",
    "    req = urllib2.Request(url)\n",
    "    page = bs(opener.open(req))\n",
    "    page_list = [i for i in re.findall('<a href=\"(.*?)>', str(page)) if 'bookmarks?' in i]\n",
    "    page_list = sorted(list(set([i.split()[0].replace('\"', '') for i in page_list])))       \n",
    "        \n",
    "    dt = re.findall('<p class=\"datetime\">(.*?)</p>', str(page))\n",
    "    times = []\n",
    "    month_dict = {'Jan':'01', 'Feb':'02','Mar':'03', 'Apr':'04', 'May':'05', 'Jun':'06', 'Jul':'07', 'Aug':'08', 'Sep':'09', 'Oct':'10', 'Nov':'11', 'Dec':'12'}\n",
    "    for time in dt:\n",
    "        times.append(time.split()[2] + '-' + month_dict.get(time.split()[1]))\n",
    "    times = times[1:]\n",
    "    if page_list != []:\n",
    "        for page in page_list:\n",
    "            times += get_bookmarks_time_subpages('http://archiveofourown.org'+page, opener=opener)\n",
    "    c = Counter(times)\n",
    "    return {time:c[time] for time in times}\n",
    "\n",
    "def get_bookmarks_time_subpages(url, opener=opener):\n",
    "    #A work's bookmarks can take up multiple pages. In this case, all timestamp information is add to the first page.\n",
    "    req = urllib2.Request(url)\n",
    "    page = bs(opener.open(req))\n",
    "    dt = re.findall('<p class=\"datetime\">(.*?)</p>', str(page))\n",
    "    times = []\n",
    "    month_dict = {'Jan':'01', 'Feb':'02','Mar':'03', 'Apr':'04', 'May':'05', 'Jun':'06', 'Jul':'07', 'Aug':'08', 'Sep':'09', 'Oct':'10', 'Nov':'11', 'Dec':'12'}\n",
    "    for time in dt:\n",
    "        times.append(time.split()[2] + '-' + month_dict.get(time.split()[1]))\n",
    "    return times[1:]\n",
    "\n",
    "def write_header(outfile):\n",
    "    f = open(outfile, 'a')\n",
    "    writer = csv.writer(f, delimiter=',')\n",
    "    keys = ['AdditionalTags', 'ArchiveWarnings', 'Author', 'Bookmarks', 'Category', 'ChapterIndex', 'Chapters', 'Characters', 'Comments', 'CompleteDate', 'Fandoms', 'Hits', 'Kudos', 'Language', 'Notes', 'PublishDate', 'Rating', 'Relationship', 'Summary', 'Text', 'Title', 'UpdateDate', 'URL', 'Words']\n",
    "    writer.writerow(keys)\n",
    "    f.close()\n",
    "\n",
    "def write_work_content(work_dict,outfile):\n",
    "    #write work metadata and contents as values of a sorted dictionary.\n",
    "    f = open(outfile, 'a')\n",
    "    writer = csv.writer(f, delimiter=',')\n",
    "    writer.writerow(OrderedDict(sorted(work_dict.items())).values())\n",
    "    f.close()\n",
    "\n",
    "def create_work_dict(url, contents):\n",
    "#     get work metadata and contents into a dictionary.\n",
    "    work = {}\n",
    "    try:\n",
    "        rating = re.findall('Rating:(.*?)<br />',contents) \n",
    "        warning = re.findall('Warnings:(.*?)<br />',contents)\n",
    "        fandom = re.findall(r'Fandoms:\\\\n          \\\\n\\\\n\\\\n(.*?)\\\\n\\\\n\\\\n\\\\n|Fandom:\\\\n          \\\\n\\\\n\\\\n(.*?)\\\\n\\\\n\\\\n\\\\n',contents)\n",
    "        category = re.findall(r'Categories:\\\\n          \\\\n\\\\n\\\\n(.*?)\\\\n\\\\n\\\\n\\\\n|Category:\\\\n          \\\\n\\\\n\\\\n(.*?)\\\\n\\\\n\\\\n\\\\n',contents)\n",
    "        relationship = re.findall('Relationships:(.*?)<br />',contents)\n",
    "        characters = re.findall('Characters:(.*?)<br />',contents)\n",
    "        additional = re.findall('Additional Tags:(.*?)<br />',contents)\n",
    "        language = re.findall(r'Language:\\\\n      \\\\n\\\\n        (.*?)\\\\n      \\\\n',contents)\n",
    "        author = re.findall(r'<strong>(.*?)</strong>',contents)[1]\n",
    "        text = re.findall(r'Work Text:(.*?)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n|Chapter Text\\\\n(.*?)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n|Chapter Text\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n(.*?)',contents)\n",
    "        text = [i for i in text[0] if i != ''] if text != [] else []\n",
    "        title = re.findall(r'<strong>(.*?)</strong>',contents)[0]\n",
    "        summary = re.findall('>Summary: <p>(.*?)</p>',contents)\n",
    "        notes = re.findall(r'Notes:\\\\n\\\\n(.*?)\\\\n\\\\n',contents)\n",
    "        publishdate = re.findall('Published:([0-9]*-[0-9]*-[0-9]*)',contents)\n",
    "        completedate = re.findall('Completed:([0-9]*-[0-9]*-[0-9]*)',contents)\n",
    "        updatedate = re.findall('Updated:([0-9]*-[0-9]*-[0-9]*)',contents)\n",
    "        words = re.findall('Words:([0-9]*)',contents)\n",
    "        chapters = re.findall('Chapters:([0-9]*/[0-9]*)',contents)\n",
    "        kudos = re.findall('Kudos:([0-9]*)',contents)\n",
    "        hits = re.findall('Hits:([0-9]*)',contents)  \n",
    "        comments = re.findall('Comments:([0-9]*)',contents)\n",
    "        bookmarks = re.findall('Bookmarks:([0-9]*)',contents)\n",
    "        url = url\n",
    "\n",
    "        work['Rating'] = rating[0] if rating != [] else ''\n",
    "        work['ArchiveWarnings'] = warning[0] if warning != [] else ''\n",
    "        work['Fandoms'] = [i for i in fandom[0] if i != ''][0] if fandom != [] else ''\n",
    "        work['Category'] = [i for i in category[0] if i != ''][0] if category != [] else ''\n",
    "        work['Relationship'] = relationship[0] if relationship != [] else '' \n",
    "        work['Characters'] = characters[0] if characters != [] else ''\n",
    "        work['AdditionalTags'] = additional[0] if additional != [] else ''\n",
    "        work['Language'] = language[0] if language != [] else ''\n",
    "        work['Author'] = author\n",
    "        work['Text']= text[0] if text != [] else ''\n",
    "        work['Title']  = title\n",
    "        work['Summary'] = summary[0] if summary != [] else ''\n",
    "        work['Notes'] = notes[0] if notes != [] else ''\n",
    "        work['PublishDate'] = publishdate[0] if publishdate != [] else ''\n",
    "        work['CompleteDate'] = completedate[0] if completedate != [] else ''\n",
    "        work['UpdateDate'] = updatedate[0] if updatedate != [] else ''\n",
    "        work['Words'] = words[0] if words != [] else ''\n",
    "        work['Chapters'] = chapters[0] if chapters != [] else ''\n",
    "        work['Kudos'] = kudos[0] if kudos != [] else ''\n",
    "        work['Hits'] = hits[0] if hits != [] else ''\n",
    "        work['Comments'] = comments[0] if comments != [] else ''\n",
    "        work['Bookmarks'] = bookmarks[0] if bookmarks != [] else ''\n",
    "\n",
    "        #For a single-chapter work, there is no complete date. In this case, fill in with publish date.\n",
    "        if len(work['Chapters']) > 2:\n",
    "            if work['Chapters'][2]== '1':\n",
    "                work['CompleteDate'] = work['PublishDate']\n",
    "\n",
    "        #Find comments-timestamps for single-chapter work.\n",
    "        if work['Comments'] > 0 and 'works' in url:\n",
    "            id = [i for i in re.findall('[0-9]*', url) if i != ''][0]\n",
    "            comments_url = 'http://archiveofourown.org/comments/show_comments?work_id=' + str(id) \n",
    "            work['Comments'] = get_comments_time(comments_url, opener=opener)\n",
    "            if work['Comments'] == {}:\n",
    "                work['Comments'] = ''\n",
    "\n",
    "        #Find comments-timestamps for multi-chapter work.\n",
    "        if work['Comments'] > 0 and 'chapters' in url:\n",
    "            id = [i for i in re.findall('[0-9]*', url) if i != ''][1]\n",
    "            comments_url = 'http://archiveofourown.org/comments/show_comments?chapter_id=' + str(id) \n",
    "            work['Comments'] = get_comments_time(comments_url, opener=opener)\n",
    "            if work['Comments'] == {}:\n",
    "                work['Comments'] = ''\n",
    "\n",
    "        #Find bookmarks-timestamps for all works.\n",
    "        if work['Bookmarks'] > 0:\n",
    "            id = [i for i in re.findall('[0-9]*', url) if i != ''][0]        \n",
    "            bookmarks_url = 'http://archiveofourown.org/works/' + id + '/bookmarks'\n",
    "            work['Bookmarks'] = get_bookmarks_time(bookmarks_url)\n",
    "            if work['Bookmarks'] == {}:\n",
    "                work['Bookmarks'] = ''\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "    return work\n",
    "\n",
    "def get_chapters_list(url,opener=opener):\n",
    "    #Find chapters urls and publish time for the chapter by going to the navigate page.\n",
    "    #Returns tuple (chapter url, time)\n",
    "    url_full = show_full_contents(url)\n",
    "    chapters_list = []\n",
    "    navigate = ''\n",
    "    \n",
    "    try:\n",
    "        req = urllib2.Request(url_full)\n",
    "        page = bs(opener.open(req))\n",
    "        for link in page.find_all('a'):\n",
    "            if 'Chapter Index' in link.text and len(link.get('href')) > 1:\n",
    "                navigate = 'http://archiveofourown.org' + link.get('href')\n",
    "\n",
    "        if navigate != '':\n",
    "            req2 = urllib2.Request(navigate)\n",
    "            page2 = bs(opener.open(req2))\n",
    "            links = re.findall('<li><a href=\"(.*?)</span></li>', str(page2))\n",
    "            for i in links:\n",
    "                chapter_url = 'http://archiveofourown.org' + i.split('\"')[0]\n",
    "                chapter_index = re.findall('[0-9]+\\.', i) [0].replace('.', '')\n",
    "                chapter_time = re.findall('<span class=\"datetime\">\\((.*?)\\)', i)[0]\n",
    "                chapters_list.append((chapter_url, chapter_index, chapter_time))\n",
    "    except:\n",
    "        pass\n",
    "    return chapters_list\n",
    "\n",
    "def read_single_work(url):\n",
    "    #Retrieve information from single-chapter work\n",
    "    try:\n",
    "        url_full = show_full_contents(url)\n",
    "        c = get_contents(url_full)\n",
    "        work = create_work_dict(url_full, str(c))\n",
    "        work['ChapterIndex'] = ''\n",
    "        write_work_content(work,outfile)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "def read_chapter(url, idx, time):\n",
    "    #Retrieve information from multi-chapter work. \n",
    "    #In this case, the publish time is replaced with the publish time for each chapter, but the complete time\n",
    "    #and update time is still for the work as a whole.\n",
    "    try:\n",
    "        url_full = show_full_contents(url)\n",
    "        c = get_contents(url_full)\n",
    "        work = create_work_dict(url_full, str(c))\n",
    "        work['PublishDate'] = time\n",
    "        work['ChapterIndex'] = idx\n",
    "        write_work_content(work,outfile)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "#main loop\n",
    "def run_scraper():\n",
    "    write_header(outfile)   \n",
    "    for i in range(start_page, max_page+1):\n",
    "        try:\n",
    "            print 'start crawling page:', i\n",
    "            page = find_page(start, i)\n",
    "            worklist = find_works(page)\n",
    "            for w in worklist:\n",
    "                ch_list_time = get_chapters_list(w)\n",
    "                if ch_list_time != []:\n",
    "                    for ch in ch_list_time:\n",
    "                        read_chapter(ch[0], ch[1], ch[2])\n",
    "                else:\n",
    "                    read_single_work(w)\n",
    "            print 'finished crawling page:', i\n",
    "            with open(logfile, 'a') as g:\n",
    "                g.write(str(i) + '\\n')\n",
    "        except:\n",
    "            print 'failed to retrieve from page'\n",
    "            pass\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    randsleep = 0.001 * random.randrange(1, 100)\n",
    "    time.sleep(randsleep)\n",
    "    run_scraper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
