{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import urllib2\n",
    "import re\n",
    "import csv\n",
    "from collections import OrderedDict, Counter\n",
    "import cookielib\n",
    "import time\n",
    "import cProfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start = 'http://archiveofourown.org/tags/Sherlock%20(TV)/works'\n",
    "outfile = './sherlock_6-10.csv'\n",
    "start_page = 6\n",
    "max_page = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cookie_file = './cookie'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_cookie(cookie_file):\n",
    "    cookie = cookielib.MozillaCookieJar(cookie_file)\n",
    "    opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(cookie))\n",
    "    response = opener.open('http://archiveofourown.org/works/5051548?view_adult=true')\n",
    "    cookie.save(ignore_discard=True, ignore_expires=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_cookie(cookie_file):\n",
    "    cookie = cookielib.MozillaCookieJar()\n",
    "    cookie.load(cookie_file, ignore_discard=True, ignore_expires=True)\n",
    "    return cookie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save_cookie(cookie_file)\n",
    "cookie = load_cookie(cookie_file)\n",
    "opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(cookie))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_page(base_url, page_number):\n",
    "    #go to any page number.\n",
    "    return base_url+'?page=' +str(page_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def find_works(page):\n",
    "    #Find all works from a works list page.\n",
    "    works_page = bs(urllib2.urlopen(page))\n",
    "    links = []\n",
    "    for link in works_page.find_all('a'):\n",
    "        try:\n",
    "            url = link.get('href')\n",
    "            url_s = [i for i in url.split('/') if i != '']\n",
    "            if 'work' in url and len(url_s) == 2 and str(url_s[1]).isdigit():\n",
    "                    links.append('http://archiveofourown.org'+link.get('href'))\n",
    "        except:\n",
    "            pass\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def show_full_contents(url):\n",
    "    #go through adult contents filtering.\n",
    "    base = bs(urllib2.urlopen(url))\n",
    "    full_url = url\n",
    "    for link in base.find_all('a'):\n",
    "        if 'Proceed' in link.text:\n",
    "            full_url = url +'?view_adult=true'\n",
    "    return full_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_contents(url, opener=opener):\n",
    "#     get work metadata and contents from the work page.\n",
    "    try:\n",
    "        req = urllib2.Request(url)\n",
    "        page = bs(opener.open(req))\n",
    "        contents = str(page.body.text.encode('utf-8'))\n",
    "    except:\n",
    "        contents = ''\n",
    "        pass\n",
    "    return url, contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_comments_time(url, opener=opener):\n",
    "    #go to the comments page of the work and find the timestamps for the comments\n",
    "    #returns a dict of {month:# of comments in the month}\n",
    "    req = urllib2.Request(url)\n",
    "    page = bs(opener.open(req))\n",
    "    times = []\n",
    "    month_dict = {'Jan':'01', 'Feb':'02','Mar':'03', 'Apr':'04', 'May':'05', 'Jun':'06', 'Jul':'07', 'Aug':'08', 'Sep':'09', 'Oct':'10', 'Nov':'11', 'Dec':'12'}\n",
    "    for line in str(page).split('<span class=\"posted datetime\">'):\n",
    "        month = re.findall('Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec', line)[0]\n",
    "        year = re.findall('<span class=\"year\">([0-9]*)</span>', line)\n",
    "        if month != [] and year != []:\n",
    "            times.append(str(year[0]) + '-' + month_dict.get(month))\n",
    "    c = Counter(times)\n",
    "    times_dict = {time:c[time] for time in times}\n",
    "    return times_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_bookmarks_time(url, opener=opener):\n",
    "    #go to the bookmarks page of the work and find the timestamps for the bookmarks\n",
    "    #returns a dict of {month:# of bookmarks in the month}\n",
    "\n",
    "    req = urllib2.Request(url)\n",
    "    page = bs(opener.open(req))\n",
    "    page_list = [i for i in re.findall('<a href=\"(.*?)>', str(page)) if 'bookmarks?' in i]\n",
    "    page_list = sorted(list(set([i.split()[0].replace('\\\"', '') for i in page_list])))       \n",
    "        \n",
    "    dt = re.findall('<p class=\"datetime\">(.*?)</p>', str(page))\n",
    "    times = []\n",
    "    month_dict = {'Jan':'01', 'Feb':'02','Mar':'03', 'Apr':'04', 'May':'05', 'Jun':'06', 'Jul':'07', 'Aug':'08', 'Sep':'09', 'Oct':'10', 'Nov':'11', 'Dec':'12'}\n",
    "    for time in dt:\n",
    "        times.append(time.split()[2] + '-' + month_dict.get(time.split()[1]))\n",
    "    times = times[1:]\n",
    "    if page_list != []:\n",
    "        for page in page_list:\n",
    "            times += get_bookmarks_time_subpages('http://archiveofourown.org'+page, opener=opener)\n",
    "        \n",
    "    c = Counter(times)\n",
    "    times_dict = {time:c[time] for time in times}\n",
    "    return times_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_bookmarks_time_subpages(url, opener=opener):\n",
    "    #A work's bookmarks can take up multiple pages. In this case, all timestamp information is add to the first page.\n",
    "    req = urllib2.Request(url)\n",
    "    page = bs(opener.open(req))\n",
    "    dt = re.findall('<p class=\"datetime\">(.*?)</p>', str(page))\n",
    "    times = []\n",
    "    month_dict = {'Jan':'01', 'Feb':'02','Mar':'03', 'Apr':'04', 'May':'05', 'Jun':'06', 'Jul':'07', 'Aug':'08', 'Sep':'09', 'Oct':'10', 'Nov':'11', 'Dec':'12'}\n",
    "    for time in dt:\n",
    "        times.append(time.split()[2] + '-' + month_dict.get(time.split()[1]))\n",
    "    times = times[1:]\n",
    "    return times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def write_header(outfile):\n",
    "    f = open(outfile, 'a')\n",
    "    writer = csv.writer(f, delimiter=',')\n",
    "    keys = ['AdditionalTags', 'ArchiveWarnings', 'Author', 'Bookmarks', 'Category', 'ChapterIndex', 'Chapters', 'Characters',\\\n",
    "             'Comments', 'CompleteDate', 'Fandoms', 'Hits', 'Kudos', 'Language', 'Notes', 'PublishDate', 'Rating',\\\n",
    "             'Relationship', 'Summary', 'Text', 'Title', 'UpdateDate', 'Words']\n",
    "    writer.writerow(keys)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def write_work_content(work_dict,outfile):\n",
    "    #write work metadata and contents as values of a sorted dictionary.\n",
    "    f = open(outfile, 'a')\n",
    "    writer = csv.writer(f, delimiter=',')\n",
    "    writer.writerow(OrderedDict(sorted(work_dict.items())).values())\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#creates dictionary for information in a single work.\n",
    "def create_work_dict(url, contents):\n",
    "#     get work metadata and contents into a dictionary.\n",
    "\n",
    "    work = {}\n",
    "    \n",
    "    try:\n",
    "        rating = re.findall('Rating:(.*?)<br />',contents) \n",
    "        warning = re.findall('Warnings:(.*?)<br />',contents)\n",
    "        fandom = re.findall(r'Fandoms:\\\\n          \\\\n\\\\n\\\\n(.*?)\\\\n\\\\n\\\\n\\\\n|Fandom:\\\\n          \\\\n\\\\n\\\\n(.*?)\\\\n\\\\n\\\\n\\\\n',contents)\n",
    "        category = re.findall(r'Categories:\\\\n          \\\\n\\\\n\\\\n(.*?)\\\\n\\\\n\\\\n\\\\n|Category:\\\\n          \\\\n\\\\n\\\\n(.*?)\\\\n\\\\n\\\\n\\\\n',contents)\n",
    "        relationship = re.findall('Relationships:(.*?)<br />',contents)\n",
    "        characters = re.findall('Characters:(.*?)<br />',contents)\n",
    "        additional = re.findall('Additional Tags:(.*?)<br />',contents)\n",
    "        language = re.findall(r'Language:\\\\n      \\\\n\\\\n        (.*?)\\\\n      \\\\n',contents)\n",
    "        author = re.findall(r'<strong>(.*?)</strong>',contents)[1]\n",
    "        text = re.findall(r'Work Text:(.*?)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n|Chapter Text\\\\n(.*?)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n|Chapter Text\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n(.*?)',contents)\n",
    "        text = [i for i in text[0] if i != ''] if text != [] else []\n",
    "        title = re.findall(r'<strong>(.*?)</strong>',contents)[0]\n",
    "        summary = re.findall('>Summary: <p>(.*?)</p>',contents)\n",
    "        notes = re.findall(r'Notes:\\\\n\\\\n(.*?)\\\\n\\\\n',contents)\n",
    "        publishdate = re.findall('Published:([0-9]*-[0-9]*-[0-9]*)',contents)\n",
    "        completedate = re.findall('Completed:([0-9]*-[0-9]*-[0-9]*)',contents)\n",
    "        updatedate = re.findall('Updated:([0-9]*-[0-9]*-[0-9]*)',contents)\n",
    "        words = re.findall('Words:([0-9]*)',contents)\n",
    "        chapters = re.findall('Chapters:([0-9]*/[0-9]*)',contents)\n",
    "        kudos = re.findall('Kudos:([0-9]*)',contents)\n",
    "        hits = re.findall('Hits:([0-9]*)',contents)  \n",
    "        comments = re.findall('Comments:([0-9]*)',contents)\n",
    "        bookmarks = re.findall('Bookmarks:([0-9]*)',contents)\n",
    "\n",
    "\n",
    "        work['Rating'] = rating[0] if rating != [] else ''\n",
    "        work['ArchiveWarnings'] = warning[0] if warning != [] else ''\n",
    "        work['Fandoms'] = [i for i in fandom[0] if i != ''][0] if fandom != [] else ''\n",
    "        work['Category'] = [i for i in category[0] if i != ''][0] if category != [] else ''\n",
    "        work['Relationship'] = relationship[0] if relationship != [] else '' \n",
    "        work['Characters'] = characters[0] if characters != [] else ''\n",
    "        work['AdditionalTags'] = additional[0] if additional != [] else ''\n",
    "        work['Language'] = language[0] if language != [] else ''\n",
    "        work['Author'] = author\n",
    "        work['Text']= text[0] if text != [] else ''\n",
    "        work['Title']  = title\n",
    "        work['Summary'] = summary[0] if summary != [] else ''\n",
    "        work['Notes'] = notes[0] if notes != [] else ''\n",
    "        work['PublishDate'] = publishdate[0] if publishdate != [] else ''\n",
    "        work['CompleteDate'] = completedate[0] if completedate != [] else ''\n",
    "        work['UpdateDate'] = updatedate[0] if updatedate != [] else ''\n",
    "        work['Words'] = words[0] if words != [] else ''\n",
    "        work['Chapters'] = chapters[0] if chapters != [] else ''\n",
    "        work['Kudos'] = kudos[0] if kudos != [] else ''\n",
    "        work['Hits'] = hits[0] if hits != [] else ''\n",
    "        work['Comments'] = comments[0] if comments != [] else ''\n",
    "        work['Bookmarks'] = bookmarks[0] if bookmarks != [] else ''\n",
    "\n",
    "        #For a single-chapter work, there is no complete date. In this case, fill in with publish date.\n",
    "        if len(work['Chapters']) > 2:\n",
    "            if work['Chapters'][2]== '1':\n",
    "                work['CompleteDate'] = work['PublishDate']\n",
    "\n",
    "        #Find comments-timestamps for single-chapter work.\n",
    "        if work['Comments'] > 0 and 'works' in url:\n",
    "            id = [i for i in re.findall('[0-9]*', url) if i != ''][0]\n",
    "            comments_url = 'http://archiveofourown.org/comments/show_comments?work_id=' + str(id) \n",
    "            work['Comments'] = get_comments_time(comments_url, opener=opener)\n",
    "            if work['Comments'] == {}:\n",
    "                work['Comments'] = ''\n",
    "            \n",
    "        #Find comments-timestamps for multi-chapter work.\n",
    "        if work['Comments'] > 0 and 'chapters' in url:\n",
    "            id = [i for i in re.findall('[0-9]*', url) if i != ''][1]\n",
    "            comments_url = 'http://archiveofourown.org/comments/show_comments?chapter_id=' + str(id) \n",
    "            work['Comments'] = get_comments_time(comments_url, opener=opener)\n",
    "            if work['Comments'] == {}:\n",
    "                work['Comments'] = ''\n",
    "        \n",
    "        #Find bookmarks-timestamps for all works.\n",
    "        if work['Bookmarks'] > 0:\n",
    "            id = [i for i in re.findall('[0-9]*', url) if i != ''][0]        \n",
    "            bookmarks_url = 'http://archiveofourown.org/works/' + id + '/bookmarks'\n",
    "            work['Bookmarks'] = get_bookmarks_time(bookmarks_url)\n",
    "            if work['Bookmarks'] == {}:\n",
    "                work['Bookmarks'] = ''\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "    return work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# content = get_contents('http://archiveofourown.org/works/5772715/chapters/13303756')\n",
    "# w =  create_work_dict('http://archiveofourown.org/works/5772715/chapters/13303756', str(content))\n",
    "# for i in w:\n",
    "#     print i, w[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_chapters_list(url,opener=opener):\n",
    "    #Find chapters urls and publish time for the chapter by going to the navigate page.\n",
    "    #Returns tuple (chapter url, time)\n",
    "    url_full = show_full_contents(url)\n",
    "    chapters_list = []\n",
    "    navigate = ''\n",
    "    \n",
    "    req = urllib2.Request(url_full)\n",
    "    page = bs(opener.open(req))\n",
    "    for link in page.find_all('a'):\n",
    "        if 'Chapter Index' in link.text and len(link.get('href')) > 1:\n",
    "            navigate = 'http://archiveofourown.org' + link.get('href')\n",
    "    \n",
    "    if navigate != '':\n",
    "        req2 = urllib2.Request(navigate)\n",
    "        page2 = bs(opener.open(req2))\n",
    "#         print page2\n",
    "        \n",
    "        links = re.findall('<li><a href=\"(.*?)</span></li>', str(page2))\n",
    "        for i in links:\n",
    "            chapter_url = 'http://archiveofourown.org' + i.split('\\\"')[0]\n",
    "            chapter_index = re.findall('[0-9]+\\.', i) [0].replace('.', '')\n",
    "            chapter_time = re.findall('<span class=\"datetime\">\\((.*?)\\)', i)[0]\n",
    "            chapters_list.append((chapter_url, chapter_index, chapter_time))\n",
    "            \n",
    "    return chapters_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get_chapters_list('http://archiveofourown.org/works/5687074/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_single_work(url):\n",
    "    #Retrieve information from single-chapter work\n",
    "    url_full = show_full_contents(url)\n",
    "    c = get_contents(url_full)\n",
    "    work = create_work_dict(url_full, str(c))\n",
    "    work['ChapterIndex'] = ''\n",
    "    write_work_content(work,outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_chapter(url, idx, time):\n",
    "    #Retrieve information from multi-chapter work. \n",
    "    #In this case, the publish time is replaced with the publish time for each chapter, but the complete time\n",
    "    #and update time is still for the work as a whole.\n",
    "    url_full = show_full_contents(url)\n",
    "    c = get_contents(url_full)\n",
    "    work = create_work_dict(url_full, str(c))\n",
    "    work['PublishDate'] = time\n",
    "    work['ChapterIndex'] = idx\n",
    "    write_work_content(work,outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# s = get_bookmarks_time('http://archiveofourown.org/works/5951704/bookmarks')\n",
    "# sum(s.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get_next_chapter('http://archiveofourown.org/works/3078407?view_adult=true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# s = get_contents('http://archiveofourown.org/works/5051548?view_adult=true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# d = create_work_dict('u',str(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# d = create_work_dict('http://archiveofourown.org/works/5205566',str(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# c = bs(urllib2.urlopen('http://archiveofourown.org/works/5051548?view_adult=true'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# req2 = urllib2.Request('http://archiveofourown.org/works/5051548/navigate')\n",
    "# page2 = bs(opener.open(req2))\n",
    "# for link in page2.find_all('a'):\n",
    "#     if 'Chapter' in link.text:\n",
    "#         print link.text, link.get('href')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ch = get_chapters_list('http://archiveofourown.org/works/5144414/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#main loop\n",
    "def run_scraper():\n",
    "    write_header(outfile)\n",
    "    start_time = time.time()\n",
    "    count = 0\n",
    "    try:\n",
    "        for i in range(start_page, max_page+1):\n",
    "            print 'start crawling page:', i\n",
    "            page = find_page(start, i)\n",
    "            worklist = find_works(page)\n",
    "            for w in worklist:\n",
    "                ch_list_time = get_chapters_list(w)\n",
    "                if ch_list_time != []:\n",
    "                    for ch in ch_list_time:\n",
    "                        ch_url = ch[0]\n",
    "                        ch_idx = ch[1]\n",
    "                        ch_time = ch[2]\n",
    "                        read_chapter(ch_url, ch_idx, ch_time)\n",
    "                else:\n",
    "                    read_single_work(w)\n",
    "                count += 1\n",
    "\n",
    "            print 'finished crawling page:', i\n",
    "    except:\n",
    "        time.sleep(5)\n",
    "        pass\n",
    "\n",
    "    print 'Saved %s works from %s pages of tag %s in %s seconds .' %(count, i, 'Sherlock Holmes', str(time.time() - start_time))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "run_scraper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start crawling page: 1\n",
      "finished crawling page: 1\n",
      "start crawling page: 2\n",
      "finished crawling page: 2\n",
      "Saved 2 works from 2 pages of tag Sherlock Holmes in 83.6192779541 seconds .\n",
      "         7311023 function calls (7190955 primitive calls) in 83.627 seconds\n",
      "\n",
      "   Ordered by: standard name\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        2    0.000    0.000    0.000    0.000 <ipython-input-101-d76bdd70dd39>:1(find_page)\n",
      "        2    0.003    0.002    2.709    1.354 <ipython-input-102-c15a0ce381ee>:1(find_works)\n",
      "       31    0.003    0.000   12.208    0.394 <ipython-input-103-4c5a61fd1ebf>:1(show_full_contents)\n",
      "       29    0.001    0.000   13.234    0.456 <ipython-input-104-8db5c777b52c>:1(get_contents)\n",
      "       58    0.003    0.000   41.877    0.722 <ipython-input-105-3c3e9de73407>:1(get_comments_time)\n",
      "       58    0.000    0.000    0.000    0.000 <ipython-input-105-3c3e9de73407>:14(<dictcomp>)\n",
      "       29    0.002    0.000   11.020    0.380 <ipython-input-106-6f6722629039>:1(get_bookmarks_time)\n",
      "       29    0.000    0.000    0.000    0.000 <ipython-input-106-6f6722629039>:21(<dictcomp>)\n",
      "        1    0.000    0.000    0.008    0.008 <ipython-input-108-ea110dfe10eb>:1(write_header)\n",
      "       29    0.000    0.000    0.013    0.000 <ipython-input-109-e68a9b3841d9>:1(write_work_content)\n",
      "       29    0.003    0.000   52.989    1.827 <ipython-input-110-b1cdf4e711b0>:2(create_work_dict)\n",
      "        2    0.000    0.000    3.314    1.657 <ipython-input-112-4c31cb8b5c4e>:1(get_chapters_list)\n",
      "       29    0.004    0.000   77.594    2.676 <ipython-input-115-f192a3601cac>:1(read_chapter)\n",
      "        1    0.001    0.001   83.627   83.627 <ipython-input-126-07baeb51210d>:2(run_scraper)\n",
      "        1    0.000    0.000   83.628   83.628 <string>:1(<module>)\n",
      "     1376    0.002    0.000    0.003    0.000 <string>:8(__new__)\n",
      "    80529    0.045    0.000    0.045    0.000 __init__.py:101(can_be_empty_element)\n",
      "      306    0.000    0.000    0.000    0.000 __init__.py:121(deprecated_argument)\n",
      "    54839    0.204    0.000    0.338    0.000 __init__.py:145(_replace_cdata_list_attribute_values)\n",
      "      153    0.001    0.000    5.940    0.039 __init__.py:206(_feed)\n",
      "      153    0.002    0.000    0.007    0.000 __init__.py:216(reset)\n",
      "    80376    0.122    0.000    0.145    0.000 __init__.py:242(popTag)\n",
      "    80529    0.192    0.000    0.232    0.000 __init__.py:251(pushTag)\n",
      "    80529    0.037    0.000    0.044    0.000 __init__.py:255(set_up_substitutions)\n",
      "   257070    0.556    0.000    1.338    0.000 __init__.py:260(endData)\n",
      "        8    0.000    0.000    0.000    0.000 __init__.py:274(dumps)\n",
      "   122877    0.193    0.000    0.458    0.000 __init__.py:290(object_was_parsed)\n",
      "    80376    0.349    0.000    0.569    0.000 __init__.py:301(_popToTag)\n",
      "    80376    0.359    0.000    2.178    0.000 __init__.py:324(handle_starttag)\n",
      "    80376    0.089    0.000    0.679    0.000 __init__.py:351(handle_endtag)\n",
      "   140852    0.068    0.000    0.100    0.000 __init__.py:356(handle_data)\n",
      "      118    0.001    0.000    2.179    0.018 __init__.py:359(decode)\n",
      "      153    0.003    0.000    0.004    0.000 __init__.py:38(lookup)\n",
      "      153    0.007    0.000   30.651    0.200 __init__.py:80(__init__)\n",
      "      306    0.000    0.000    0.000    0.000 __init__.py:98(reset)\n",
      "    25537    0.046    0.000    0.046    0.000 _abcoll.py:408(keys)\n",
      "       29    0.001    0.000    0.001    0.000 _abcoll.py:548(update)\n",
      "      153    0.000    0.000    0.000    0.000 _lxml.py:123(close)\n",
      "    80376    0.634    0.000    3.011    0.000 _lxml.py:126(start)\n",
      "    80376    0.017    0.000    0.017    0.000 _lxml.py:166(_prefix_for_namespace)\n",
      "    80376    0.223    0.000    1.470    0.000 _lxml.py:175(end)\n",
      "   133034    0.092    0.000    0.186    0.000 _lxml.py:194(data)\n",
      "      153    0.001    0.000    0.021    0.000 _lxml.py:197(doctype)\n",
      "     7818    0.015    0.000    0.176    0.000 _lxml.py:202(comment)\n",
      "      153    0.000    0.000    0.000    0.000 _lxml.py:218(default_parser)\n",
      "      153    0.010    0.000    5.937    0.039 _lxml.py:221(feed)\n",
      "      153    0.003    0.000    0.006    0.000 _lxml.py:45(parser_for)\n",
      "      153    0.001    0.000    0.001    0.000 _lxml.py:54(__init__)\n",
      "   244050    0.083    0.000    0.083    0.000 _lxml.py:64(_getNsTag)\n",
      "      306    0.003    0.000    0.021    0.000 _lxml.py:72(prepare_markup)\n",
      "     3314    0.003    0.000    0.003    0.000 _weakrefset.py:70(__contains__)\n",
      "     1719    0.005    0.000    0.008    0.000 abc.py:128(__instancecheck__)\n",
      "       29    0.000    0.000    0.000    0.000 abc.py:148(__subclasscheck__)\n",
      "        2    0.000    0.000    0.000    0.000 attrsettr.py:35(__getattr__)\n",
      "      208    0.002    0.000    0.002    0.000 calendar.py:610(timegm)\n",
      "       29    0.000    0.000    0.000    0.000 collections.py:109(values)\n",
      "       29    0.000    0.000    0.002    0.000 collections.py:38(__init__)\n",
      "       87    0.001    0.000    0.003    0.000 collections.py:446(__init__)\n",
      "       87    0.001    0.000    0.002    0.000 collections.py:516(update)\n",
      "      667    0.001    0.000    0.001    0.000 collections.py:59(__setitem__)\n",
      "      696    0.000    0.000    0.000    0.000 collections.py:78(__iter__)\n",
      "      150    0.000    0.000    0.000    0.000 cookielib.py:1052(set_ok_port)\n",
      "      209    0.003    0.000    0.012    0.000 cookielib.py:1073(return_ok)\n",
      "      209    0.000    0.000    0.000    0.000 cookielib.py:1091(return_ok_version)\n",
      "      209    0.000    0.000    0.004    0.000 cookielib.py:1100(return_ok_verifiability)\n",
      "      209    0.000    0.000    0.000    0.000 cookielib.py:1112(return_ok_secure)\n",
      "      209    0.000    0.000    0.001    0.000 cookielib.py:1118(return_ok_expires)\n",
      "      209    0.000    0.000    0.000    0.000 cookielib.py:1124(return_ok_port)\n",
      "      209    0.001    0.000    0.004    0.000 cookielib.py:1138(return_ok_domain)\n",
      "      209    0.001    0.000    0.007    0.000 cookielib.py:1160(domain_return_ok)\n",
      "      209    0.001    0.000    0.005    0.000 cookielib.py:1182(path_return_ok)\n",
      "      627    0.001    0.000    0.002    0.000 cookielib.py:1191(vals_sorted_by_key)\n",
      " 1254/418    0.004    0.000    0.006    0.000 cookielib.py:1196(deepvalues)\n",
      "      208    0.000    0.000    0.000    0.000 cookielib.py:123(offset_from_tz_string)\n",
      "      209    0.002    0.000    0.026    0.000 cookielib.py:1244(_cookies_for_domain)\n",
      "      209    0.001    0.000    0.027    0.000 cookielib.py:1262(_cookies_for_request)\n",
      "      209    0.002    0.000    0.004    0.000 cookielib.py:1269(_cookie_attrs)\n",
      "      209    0.000    0.000    0.000    0.000 cookielib.py:1279(<lambda>)\n",
      "      209    0.004    0.000    0.048    0.000 cookielib.py:1328(add_cookie_header)\n",
      "      416    0.004    0.000    0.005    0.000 cookielib.py:1361(_normalized_cookie_tuples)\n",
      "      208    0.003    0.000    0.007    0.000 cookielib.py:137(_str2time)\n",
      "      358    0.008    0.000    0.026    0.000 cookielib.py:1458(_cookie_from_cookie_tuple)\n",
      "      416    0.003    0.000    0.034    0.000 cookielib.py:1550(_cookies_from_attrs_set)\n",
      "      208    0.001    0.000    0.001    0.000 cookielib.py:1559(_process_rfc2109_cookies)\n",
      "      209    0.004    0.000    0.084    0.000 cookielib.py:1571(make_cookies)\n",
      "      150    0.001    0.000    0.003    0.000 cookielib.py:1638(set_cookie)\n",
      "      209    0.005    0.000    0.107    0.001 cookielib.py:1651(extract_cookies)\n",
      "      208    0.001    0.000    0.001    0.000 cookielib.py:1665(clear)\n",
      "      209    0.001    0.000    0.010    0.000 cookielib.py:1707(clear_expired_cookies)\n",
      "      209    0.000    0.000    0.000    0.000 cookielib.py:1726(__iter__)\n",
      "      208    0.003    0.000    0.015    0.000 cookielib.py:212(http2time)\n",
      "      208    0.001    0.000    0.001    0.000 cookielib.py:326(split_header_words)\n",
      "     1762    0.001    0.000    0.001    0.000 cookielib.py:43(_debug)\n",
      "      208    0.000    0.000    0.001    0.000 cookielib.py:437(_strip_quotes)\n",
      "      208    0.007    0.000    0.026    0.000 cookielib.py:444(parse_ns_headers)\n",
      "      177    0.000    0.000    0.001    0.000 cookielib.py:513(is_HDN)\n",
      "      177    0.000    0.000    0.001    0.000 cookielib.py:528(domain_match)\n",
      "      953    0.004    0.000    0.018    0.000 cookielib.py:598(request_host)\n",
      "      776    0.002    0.000    0.018    0.000 cookielib.py:614(eff_request_host)\n",
      "      359    0.002    0.000    0.007    0.000 cookielib.py:625(request_path)\n",
      "      717    0.003    0.000    0.008    0.000 cookielib.py:655(escape_path)\n",
      "      177    0.001    0.000    0.002    0.000 cookielib.py:671(reach)\n",
      "      177    0.001    0.000    0.006    0.000 cookielib.py:706(is_third_party)\n",
      "      208    0.001    0.000    0.003    0.000 cookielib.py:72(_timegm)\n",
      "      150    0.001    0.000    0.003    0.000 cookielib.py:741(__init__)\n",
      "      418    0.000    0.000    0.000    0.000 cookielib.py:790(is_expired)\n",
      "      359    0.000    0.000    0.000    0.000 cookielib.py:904(is_blocked)\n",
      "      359    0.000    0.000    0.000    0.000 cookielib.py:919(is_not_allowed)\n",
      "      150    0.002    0.000    0.011    0.000 cookielib.py:927(set_ok)\n",
      "      150    0.000    0.000    0.000    0.000 cookielib.py:946(set_ok_version)\n",
      "      150    0.000    0.000    0.004    0.000 cookielib.py:961(set_ok_verifiability)\n",
      "      150    0.000    0.000    0.000    0.000 cookielib.py:973(set_ok_name)\n",
      "      150    0.000    0.000    0.004    0.000 cookielib.py:982(set_ok_path)\n",
      "      150    0.001    0.000    0.001    0.000 cookielib.py:993(set_ok_domain)\n",
      "      150    0.000    0.000    0.000    0.000 copy.py:113(_copy_with_constructor)\n",
      "      150    0.001    0.000    0.001    0.000 copy.py:66(copy)\n",
      "   154708    0.089    0.000    0.335    0.000 dammit.py:136(substitute_xml)\n",
      "      153    0.001    0.000    0.002    0.000 dammit.py:215(__init__)\n",
      "      612    0.001    0.000    0.001    0.000 dammit.py:224(_usable)\n",
      "      306    0.002    0.000    0.015    0.000 dammit.py:232(encodings)\n",
      "      153    0.001    0.000    0.001    0.000 dammit.py:265(strip_byte_order_mark)\n",
      "      153    0.003    0.000    0.012    0.000 dammit.py:288(find_declared_encoding)\n",
      "     6358    0.006    0.000    0.007    0.000 dammit.py:92(_substitute_xml_entity)\n",
      "    63356    0.028    0.000    0.028    0.000 dammit.py:99(quoted_attribute_value)\n",
      "58646/118    0.616    0.000    2.178    0.018 element.py:1002(decode)\n",
      "58646/118    0.326    0.000    2.176    0.018 element.py:1101(decode_contents)\n",
      "   156872    0.087    0.000    0.601    0.000 element.py:114(substitute_xml)\n",
      "      182    0.001    0.000    0.040    0.000 element.py:1154(find)\n",
      "      217    0.002    0.000    0.079    0.000 element.py:1165(find_all)\n",
      "    86761    0.031    0.000    0.035    0.000 element.py:1190(descendants)\n",
      "      217    0.002    0.000    0.008    0.000 element.py:1421(__init__)\n",
      "     1159    0.003    0.000    0.010    0.000 element.py:1448(_normalize_search_value)\n",
      "      725    0.002    0.000    0.015    0.000 element.py:1484(search_tag)\n",
      "     1943    0.003    0.000    0.023    0.000 element.py:1525(search)\n",
      "   156872    0.109    0.000    0.726    0.000 element.py:153(format_string)\n",
      "      725    0.002    0.000    0.010    0.000 element.py:1551(_matches)\n",
      "      217    0.002    0.000    0.039    0.000 element.py:1609(__init__)\n",
      "      118    0.000    0.000    0.000    0.000 element.py:163(_is_xml)\n",
      "      118    0.001    0.000    0.001    0.000 element.py:179(_formatter_for_name)\n",
      "   203406    0.372    0.000    0.372    0.000 element.py:188(setup)\n",
      "     2894    0.003    0.000    0.004    0.000 element.py:258(_last_descendant)\n",
      "      217    0.005    0.000    0.077    0.000 element.py:478(_find_all)\n",
      "     4003    0.017    0.000    0.037    0.000 element.py:494(<genexpr>)\n",
      "   122877    0.122    0.000    0.311    0.000 element.py:648(__new__)\n",
      "     1218    0.001    0.000    0.001    0.000 element.py:666(__getattr__)\n",
      "    86231    0.078    0.000    0.482    0.000 element.py:677(output_ready)\n",
      "      153    0.001    0.000    0.002    0.000 element.py:69(__new__)\n",
      "     7285    0.009    0.000    0.050    0.000 element.py:696(output_ready)\n",
      "      153    0.001    0.000    0.003    0.000 element.py:725(for_name_and_ids)\n",
      "    80529    0.382    0.000    0.959    0.000 element.py:745(__init__)\n",
      "    58646    0.038    0.000    0.047    0.000 element.py:781(is_empty_element)\n",
      "      118    0.000    0.000    0.001    0.000 element.py:79(encode)\n",
      "      118    0.000    0.000    0.000    0.000 element.py:80(rewrite)\n",
      "    28300    0.022    0.000    0.044    0.000 element.py:821(_all_strings)\n",
      "     2830    0.008    0.000    0.055    0.000 element.py:846(get_text)\n",
      "     6262    0.005    0.000    0.006    0.000 element.py:890(get)\n",
      "    58646    0.029    0.000    0.041    0.000 element.py:907(__iter__)\n",
      "    99069    0.021    0.000    0.021    0.000 element.py:918(__nonzero__)\n",
      "      153    0.000    0.000    0.000    0.000 element.py:922(__setitem__)\n",
      "      182    0.002    0.000    0.042    0.000 element.py:937(__getattr__)\n",
      "      117    0.000    0.000    0.000    0.000 element.py:952(__eq__)\n",
      "      118    0.000    0.000    2.186    0.019 element.py:981(__str__)\n",
      "      118    0.000    0.000    2.185    0.019 element.py:987(encode)\n",
      "   156872    0.149    0.000    0.514    0.000 element.py:99(_substitute_if_appropriate)\n",
      "    58646    0.013    0.000    0.013    0.000 element.py:995(_should_pretty_print)\n",
      "        8    0.000    0.000    0.000    0.000 encoder.py:121(__init__)\n",
      "        8    0.000    0.000    0.000    0.000 encoder.py:249(encode)\n",
      "        8    0.000    0.000    0.000    0.000 encoder.py:278(iterencode)\n",
      "       52    0.000    0.000    0.000    0.000 encoder.py:42(encode_basestring)\n",
      "        3    0.000    0.000    0.000    0.000 encoder.py:52(replace)\n",
      "        2    0.000    0.000    0.000    0.000 hmac.py:100(_current)\n",
      "        2    0.000    0.000    0.000    0.000 hmac.py:119(hexdigest)\n",
      "        2    0.000    0.000    0.000    0.000 hmac.py:30(__init__)\n",
      "        8    0.000    0.000    0.000    0.000 hmac.py:83(update)\n",
      "        2    0.000    0.000    0.000    0.000 hmac.py:88(copy)\n",
      "     1181    0.005    0.000    0.009    0.000 httplib.py:1020(putheader)\n",
      "      243    0.001    0.000   10.897    0.045 httplib.py:1040(endheaders)\n",
      "      243    0.001    0.000   10.914    0.045 httplib.py:1055(request)\n",
      "      243    0.001    0.000    0.001    0.000 httplib.py:1059(_set_content_length)\n",
      "      243    0.004    0.000   10.913    0.045 httplib.py:1082(_send_request)\n",
      "      243    0.006    0.000   39.313    0.162 httplib.py:1099(getresponse)\n",
      "     4094    0.006    0.000    0.008    0.000 httplib.py:257(addheader)\n",
      "      243    0.040    0.000    0.124    0.001 httplib.py:271(readheaders)\n",
      "      243    0.002    0.000    0.007    0.000 httplib.py:379(__init__)\n",
      "      243    0.009    0.000   39.140    0.161 httplib.py:407(_read_status)\n",
      "      243    0.008    0.000   39.295    0.162 httplib.py:446(begin)\n",
      "      243    0.001    0.000    0.002    0.000 httplib.py:532(_check_close)\n",
      "      486    0.001    0.000    0.010    0.000 httplib.py:562(close)\n",
      "     1431    0.009    0.000   24.660    0.017 httplib.py:579(read)\n",
      "     1188    0.023    0.000   24.651    0.021 httplib.py:624(_read_chunked)\n",
      "     1729    0.024    0.000   24.595    0.014 httplib.py:682(_safe_read)\n",
      "      243    0.002    0.000    0.003    0.000 httplib.py:736(__init__)\n",
      "      243    0.001    0.000    0.001    0.000 httplib.py:781(_get_hostport)\n",
      "      243    0.000    0.000    0.000    0.000 httplib.py:800(set_debuglevel)\n",
      "      243    0.003    0.000   10.866    0.045 httplib.py:833(connect)\n",
      "      243    0.001    0.000    0.004    0.000 httplib.py:841(close)\n",
      "      243    0.007    0.000   10.893    0.045 httplib.py:855(send)\n",
      "     1424    0.001    0.000    0.001    0.000 httplib.py:875(_output)\n",
      "      243    0.002    0.000   10.896    0.045 httplib.py:882(_send_output)\n",
      "      243    0.002    0.000    0.005    0.000 httplib.py:903(putrequest)\n",
      "      153    0.000    0.000    0.000    0.000 inspect.py:142(isfunction)\n",
      "      153    0.000    0.000    0.000    0.000 inspect.py:209(iscode)\n",
      "      153    0.000    0.000    0.000    0.000 inspect.py:67(ismethod)\n",
      "      153    0.003    0.000    0.004    0.000 inspect.py:744(getargs)\n",
      "      153    0.002    0.000    0.007    0.000 inspect.py:804(getargspec)\n",
      "       20    0.000    0.000    0.000    0.000 iostream.py:102(_check_mp_mode)\n",
      "        2    0.000    0.000    0.000    0.000 iostream.py:123(_flush_from_subprocesses)\n",
      "        2    0.000    0.000    0.001    0.001 iostream.py:151(flush)\n",
      "       18    0.000    0.000    0.002    0.000 iostream.py:207(write)\n",
      "        2    0.000    0.000    0.000    0.000 iostream.py:238(_flush_buffer)\n",
      "        2    0.000    0.000    0.000    0.000 iostream.py:247(_new_buffer)\n",
      "       22    0.000    0.000    0.000    0.000 iostream.py:93(_is_master_process)\n",
      "        2    0.000    0.000    0.000    0.000 iostream.py:96(_is_master_thread)\n",
      "        8    0.000    0.000    0.000    0.000 jsonapi.py:31(dumps)\n",
      "        2    0.000    0.000    0.000    0.000 jsonutil.py:102(date_default)\n",
      "      243    0.009    0.000    0.144    0.001 mimetools.py:24(__init__)\n",
      "      243    0.003    0.000    0.005    0.000 mimetools.py:33(parsetype)\n",
      "      243    0.002    0.000    0.003    0.000 mimetools.py:50(parseplist)\n",
      "        2    0.000    0.000    0.000    0.000 poll.py:77(poll)\n",
      "        2    0.000    0.000    0.000    0.000 py3compat.py:11(no_code)\n",
      "     1199    0.001    0.000    0.129    0.000 re.py:173(findall)\n",
      "     1199    0.002    0.000    0.002    0.000 re.py:230(_compile)\n",
      "     4094    0.007    0.000    0.013    0.000 rfc822.py:202(isheader)\n",
      "     4337    0.002    0.000    0.002    0.000 rfc822.py:214(islast)\n",
      "     4337    0.002    0.000    0.002    0.000 rfc822.py:224(iscomment)\n",
      "      508    0.011    0.000    0.018    0.000 rfc822.py:233(getallmatchingheaders)\n",
      "     1458    0.002    0.000    0.004    0.000 rfc822.py:290(getheader)\n",
      "      508    0.003    0.000    0.022    0.000 rfc822.py:300(getheaders)\n",
      "       90    0.000    0.000    0.000    0.000 rfc822.py:444(__contains__)\n",
      "      243    0.002    0.000    0.125    0.001 rfc822.py:88(__init__)\n",
      "        2    0.000    0.000    0.000    0.000 session.py:195(msg_header)\n",
      "        2    0.000    0.000    0.000    0.000 session.py:200(extract_header)\n",
      "        2    0.000    0.000    0.000    0.000 session.py:441(msg_id)\n",
      "        2    0.000    0.000    0.000    0.000 session.py:493(msg_header)\n",
      "        2    0.000    0.000    0.000    0.000 session.py:496(msg)\n",
      "        2    0.000    0.000    0.000    0.000 session.py:515(sign)\n",
      "        2    0.000    0.000    0.001    0.000 session.py:530(serialize)\n",
      "        2    0.000    0.000    0.001    0.001 session.py:589(send)\n",
      "        8    0.000    0.000    0.000    0.000 session.py:84(<lambda>)\n",
      "      243    0.007    0.000    0.009    0.000 socket.py:189(__init__)\n",
      "      243    0.002    0.000    0.003    0.000 socket.py:196(close)\n",
      "      243    0.002    0.000    0.005    0.000 socket.py:216(makefile)\n",
      "      486    0.005    0.000    9.967    0.021 socket.py:227(meth)\n",
      "        2    0.000    0.000    0.000    0.000 socket.py:232(send_multipart)\n",
      "      486    0.004    0.000    0.005    0.000 socket.py:250(__init__)\n",
      "      819    0.012    0.000    0.013    0.000 socket.py:280(close)\n",
      "      486    0.001    0.000    0.004    0.000 socket.py:289(__del__)\n",
      "      486    0.001    0.000    0.001    0.000 socket.py:296(flush)\n",
      " 1972/243    0.143    0.000   24.686    0.102 socket.py:340(read)\n",
      "     5458    0.044    0.000   39.199    0.007 socket.py:410(readline)\n",
      "      243    0.008    0.000   10.863    0.045 socket.py:541(create_connection)\n",
      "        2    0.000    0.000    0.000    0.000 threading.py:1143(currentThread)\n",
      "      777    0.004    0.000    0.006    0.000 threading.py:147(acquire)\n",
      "      777    0.003    0.000    0.004    0.000 threading.py:187(release)\n",
      "     1554    0.001    0.000    0.001    0.000 threading.py:64(_note)\n",
      "        2    0.000    0.000    0.000    0.000 threading.py:974(ident)\n",
      "       26    0.000    0.000    0.000    0.000 traitlets.py:395(__get__)\n",
      "      243    0.002    0.000    0.005    0.000 urllib.py:1021(__init__)\n",
      "      661    0.000    0.000    0.000    0.000 urllib.py:1027(info)\n",
      "      243    0.000    0.000    0.000    0.000 urllib.py:1058(_is_unicode)\n",
      "      243    0.001    0.000    0.001    0.000 urllib.py:1073(unwrap)\n",
      "      243    0.001    0.000    0.002    0.000 urllib.py:1082(splittype)\n",
      "      243    0.001    0.000    0.002    0.000 urllib.py:1096(splithost)\n",
      "      243    0.001    0.000    0.002    0.000 urllib.py:1187(splittag)\n",
      "      243    0.001    0.000    0.001    0.000 urllib.py:1225(unquote)\n",
      "      717    0.002    0.000    0.003    0.000 urllib.py:1266(quote)\n",
      "      243    0.002    0.000    0.003    0.000 urllib.py:962(__init__)\n",
      "       90    0.000    0.000    0.001    0.000 urllib.py:980(close)\n",
      "      243    0.002    0.000    0.008    0.000 urllib2.py:1122(do_request_)\n",
      "      243    0.016    0.000   50.255    0.207 urllib2.py:1151(do_open)\n",
      "      243    0.000    0.000    0.000    0.000 urllib2.py:1170(<genexpr>)\n",
      "     1181    0.001    0.000    0.002    0.000 urllib2.py:1181(<genexpr>)\n",
      "      243    0.002    0.000   50.257    0.207 urllib2.py:1226(http_open)\n",
      "      209    0.001    0.000    0.049    0.000 urllib2.py:1251(http_request)\n",
      "      209    0.001    0.000    0.108    0.001 urllib2.py:1255(http_response)\n",
      "       33    0.000    0.000    7.361    0.223 urllib2.py:131(urlopen)\n",
      "      153    0.001    0.000    0.006    0.000 urllib2.py:208(request_host)\n",
      "      243    0.002    0.000    0.011    0.000 urllib2.py:226(__init__)\n",
      "       61    0.000    0.000    0.000    0.000 urllib2.py:246(__getattr__)\n",
      "      333    0.000    0.000    0.001    0.000 urllib2.py:258(get_method)\n",
      "      576    0.000    0.000    0.000    0.000 urllib2.py:269(has_data)\n",
      "     1798    0.002    0.000    0.002    0.000 urllib2.py:275(get_full_url)\n",
      "      486    0.001    0.000    0.003    0.000 urllib2.py:281(get_type)\n",
      "      486    0.001    0.000    0.004    0.000 urllib2.py:288(get_host)\n",
      "      243    0.000    0.000    0.000    0.000 urllib2.py:295(get_selector)\n",
      "      243    0.000    0.000    0.000    0.000 urllib2.py:307(has_proxy)\n",
      "      267    0.000    0.000    0.000    0.000 urllib2.py:310(get_origin_req_host)\n",
      "      359    0.000    0.000    0.000    0.000 urllib2.py:313(is_unverifiable)\n",
      "      695    0.001    0.000    0.001    0.000 urllib2.py:320(add_unredirected_header)\n",
      "      695    0.001    0.000    0.001    0.000 urllib2.py:324(has_header)\n",
      "  576/367    0.002    0.000   50.382    0.137 urllib2.py:401(_call_chain)\n",
      "  243/153    0.006    0.000   50.479    0.330 urllib2.py:413(open)\n",
      "      243    0.003    0.000   50.262    0.207 urllib2.py:441(_open)\n",
      "    90/61    0.001    0.000   21.514    0.353 urllib2.py:456(error)\n",
      "  243/153    0.001    0.000   21.515    0.141 urllib2.py:543(http_response)\n",
      "       90    0.002    0.000    0.004    0.000 urllib2.py:568(redirect_request)\n",
      "       90    0.000    0.000    0.000    0.000 urllib2.py:588(<genexpr>)\n",
      "    90/61    0.003    0.000   21.513    0.353 urllib2.py:602(http_error_302)\n",
      "     1376    0.006    0.000    0.020    0.000 urlparse.py:137(urlparse)\n",
      "      220    0.002    0.000    0.002    0.000 urlparse.py:160(_splitnetloc)\n",
      "     1735    0.007    0.000    0.012    0.000 urlparse.py:168(urlsplit)\n",
      "      180    0.000    0.000    0.001    0.000 urlparse.py:223(urlunparse)\n",
      "      180    0.001    0.000    0.001    0.000 urlparse.py:233(urlunsplit)\n",
      "       90    0.001    0.000    0.003    0.000 urlparse.py:251(urljoin)\n",
      "       11    0.000    0.000    0.001    0.000 urlparse.py:68(clear_cache)\n",
      "      960    0.001    0.000    0.003    0.000 utf_8.py:15(decode)\n",
      "        2    0.000    0.000    0.000    0.000 uuid.py:103(__init__)\n",
      "        2    0.000    0.000    0.000    0.000 uuid.py:199(__str__)\n",
      "        2    0.000    0.000    0.000    0.000 uuid.py:582(uuid4)\n",
      "      960    0.002    0.000    0.002    0.000 {_codecs.utf_8_decode}\n",
      "       30    0.000    0.000    0.000    0.000 {_csv.writer}\n",
      "      243    0.895    0.004    0.895    0.004 {_socket.getaddrinfo}\n",
      "   124932    0.147    0.000    0.147    0.000 {built-in method __new__ of type object at 0x100186920}\n",
      "      243    0.000    0.000    0.000    0.000 {built-in method fromkeys}\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method now}\n",
      "     7916    0.006    0.000    0.006    0.000 {cStringIO.StringIO}\n",
      "   275323    0.029    0.000    0.029    0.000 {callable}\n",
      "     7382    0.005    0.000    0.005    0.000 {getattr}\n",
      "     4834    0.012    0.000    0.014    0.000 {hasattr}\n",
      "   759974    0.187    0.000    0.195    0.000 {isinstance}\n",
      "    58646    0.012    0.000    0.012    0.000 {iter}\n",
      "   410214    0.086    0.000    0.086    0.000 {len}\n",
      "        2    0.000    0.000    0.000    0.000 {locals}\n",
      "      629    0.001    0.000    0.001    0.000 {map}\n",
      "     2127    0.002    0.000    0.002    0.000 {max}\n",
      "      627    0.001    0.000    0.001    0.000 {method 'acquire' of 'thread.lock' objects}\n",
      "      153    0.000    0.000    0.000    0.000 {method 'add' of 'set' objects}\n",
      "   828691    0.142    0.000    0.142    0.000 {method 'append' of 'list' objects}\n",
      "      938    0.000    0.000    0.000    0.000 {method 'capitalize' of 'str' objects}\n",
      "       19    0.001    0.000    0.001    0.000 {method 'clear' of 'dict' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {method 'close' of '_io.StringIO' objects}\n",
      "       30    0.008    0.000    0.008    0.000 {method 'close' of 'file' objects}\n",
      "      153    0.020    0.000    0.119    0.001 {method 'close' of 'lxml.etree._FeedParser' objects}\n",
      "      243    9.949    0.041    9.949    0.041 {method 'connect' of '_socket.socket' objects}\n",
      "        6    0.000    0.000    0.000    0.000 {method 'copy' of '_hashlib.HASH' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {method 'copy' of 'dict' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {method 'count' of 'list' objects}\n",
      "     1113    0.003    0.000    0.005    0.000 {method 'decode' of 'str' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {method 'digest' of '_hashlib.HASH' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "      155    0.007    0.000    0.007    0.000 {method 'encode' of 'unicode' objects}\n",
      "     5723    0.003    0.000    0.003    0.000 {method 'endswith' of 'str' objects}\n",
      "      606    0.000    0.000    0.000    0.000 {method 'extend' of 'list' objects}\n",
      "      153    1.030    0.007    5.802    0.038 {method 'feed' of 'lxml.etree._FeedParser' objects}\n",
      "     7731    0.008    0.000    0.008    0.000 {method 'find' of 'str' objects}\n",
      "     1199    0.125    0.000    0.125    0.000 {method 'findall' of '_sre.SRE_Pattern' objects}\n",
      "   129003    0.030    0.000    0.030    0.000 {method 'get' of 'dict' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {method 'getvalue' of '_io.StringIO' objects}\n",
      "     1430    0.009    0.000    0.009    0.000 {method 'getvalue' of 'cStringIO.StringO' objects}\n",
      "     7266    0.001    0.000    0.001    0.000 {method 'group' of '_sre.SRE_Match' objects}\n",
      "      361    0.001    0.000    0.001    0.000 {method 'groups' of '_sre.SRE_Match' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {method 'hexdigest' of '_hashlib.HASH' objects}\n",
      "      208    0.000    0.000    0.000    0.000 {method 'index' of 'list' objects}\n",
      "      486    0.000    0.000    0.000    0.000 {method 'index' of 'str' objects}\n",
      "      153    0.000    0.000    0.000    0.000 {method 'intersection' of 'set' objects}\n",
      "       95    0.000    0.000    0.000    0.000 {method 'isdigit' of 'str' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {method 'isoformat' of 'datetime.datetime' objects}\n",
      "     8686    0.002    0.000    0.002    0.000 {method 'isspace' of 'str' objects}\n",
      "   123195    0.054    0.000    0.054    0.000 {method 'items' of 'dict' objects}\n",
      "      243    0.000    0.000    0.000    0.000 {method 'iteritems' of 'dict' objects}\n",
      "   188410    0.090    0.000    0.090    0.000 {method 'join' of 'str' objects}\n",
      "   125562    0.034    0.000    0.034    0.000 {method 'join' of 'unicode' objects}\n",
      "    55884    0.017    0.000    0.017    0.000 {method 'keys' of 'dict' objects}\n",
      "    76142    0.038    0.000    0.038    0.000 {method 'lower' of 'str' objects}\n",
      "      306    0.000    0.000    0.000    0.000 {method 'lower' of 'unicode' objects}\n",
      "      208    0.000    0.000    0.000    0.000 {method 'lstrip' of 'str' objects}\n",
      "     1910    0.003    0.000    0.003    0.000 {method 'match' of '_sre.SRE_Pattern' objects}\n",
      "     1074    0.001    0.000    0.001    0.000 {method 'partition' of 'str' objects}\n",
      "    80799    0.023    0.000    0.023    0.000 {method 'pop' of 'list' objects}\n",
      "     5324    0.004    0.000    0.004    0.000 {method 'read' of 'cStringIO.StringO' objects}\n",
      "     4915    0.004    0.000    0.004    0.000 {method 'readline' of 'cStringIO.StringO' objects}\n",
      "     6784   63.431    0.009   63.431    0.009 {method 'recv' of '_socket.socket' objects}\n",
      "      627    0.000    0.000    0.000    0.000 {method 'release' of 'thread.lock' objects}\n",
      "      119    0.000    0.000    0.000    0.000 {method 'replace' of 'str' objects}\n",
      "      153    0.000    0.000    0.000    0.000 {method 'reverse' of 'list' objects}\n",
      "      486    0.000    0.000    0.000    0.000 {method 'rfind' of 'str' objects}\n",
      "      717    0.001    0.000    0.001    0.000 {method 'rstrip' of 'str' objects}\n",
      "     2480    0.014    0.000    0.014    0.000 {method 'search' of '_sre.SRE_Pattern' objects}\n",
      "    13094    0.009    0.000    0.009    0.000 {method 'seek' of 'cStringIO.StringO' objects}\n",
      "       14    0.000    0.000    0.000    0.000 {method 'send' of 'zmq.backend.cython.socket.Socket' objects}\n",
      "      243    0.012    0.000    0.012    0.000 {method 'sendall' of '_socket.socket' objects}\n",
      "      836    0.001    0.000    0.001    0.000 {method 'sort' of 'list' objects}\n",
      "    30819    0.059    0.000    0.059    0.000 {method 'split' of '_sre.SRE_Pattern' objects}\n",
      "     3162    0.006    0.000    0.006    0.000 {method 'split' of 'str' objects}\n",
      "     2438    0.002    0.000    0.002    0.000 {method 'startswith' of 'str' objects}\n",
      "     9315    0.004    0.000    0.004    0.000 {method 'strip' of 'str' objects}\n",
      "   156909    0.246    0.000    0.253    0.000 {method 'sub' of '_sre.SRE_Pattern' objects}\n",
      "     7731    0.002    0.000    0.002    0.000 {method 'tell' of 'cStringIO.StringO' objects}\n",
      "      938    0.001    0.000    0.001    0.000 {method 'title' of 'str' objects}\n",
      "      208    0.000    0.000    0.000    0.000 {method 'toordinal' of 'datetime.date' objects}\n",
      "       10    0.000    0.000    0.000    0.000 {method 'update' of '_hashlib.HASH' objects}\n",
      "      243    0.000    0.000    0.000    0.000 {method 'update' of 'dict' objects}\n",
      "      453    0.000    0.000    0.000    0.000 {method 'upper' of 'str' objects}\n",
      "      209    0.000    0.000    0.000    0.000 {method 'values' of 'dict' objects}\n",
      "       18    0.000    0.000    0.000    0.000 {method 'write' of '_io.StringIO' objects}\n",
      "    12754    0.123    0.000    0.123    0.000 {method 'write' of 'cStringIO.StringO' objects}\n",
      "       30    0.008    0.000    0.008    0.000 {method 'writerow' of '_csv.writer' objects}\n",
      "     2184    0.004    0.000    0.004    0.000 {min}\n",
      "     2096    0.001    0.000    0.002    0.000 {next}\n",
      "       30    0.001    0.000    0.001    0.000 {open}\n",
      "       24    0.000    0.000    0.000    0.000 {posix.getpid}\n",
      "        2    0.000    0.000    0.000    0.000 {posix.urandom}\n",
      "    80775    0.064    0.000    0.064    0.000 {range}\n",
      "     2916    0.002    0.000    0.002    0.000 {setattr}\n",
      "    41754    0.035    0.000    0.035    0.000 {sorted}\n",
      "     1556    0.001    0.000    0.001    0.000 {thread.get_ident}\n",
      "      647    0.000    0.000    0.000    0.000 {time.time}\n",
      "        2    0.000    0.000    0.000    0.000 {zmq.backend.cython._poll.zmq_poll}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cProfile.run('run_scraper()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Saved 40 works from 2 pages of tag Sherlock Holmes in 758.977619886 seconds .\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
