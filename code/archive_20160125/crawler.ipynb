{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import urllib2\n",
    "import re\n",
    "import csv\n",
    "from collections import OrderedDict\n",
    "import cookielib\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start = 'http://archiveofourown.org/tags/Sherlock%20(TV)/works'\n",
    "outfile = './ao3_work_sherlock_all_1000_2000.csv'\n",
    "max_page = 3870"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cookie_file = './cookie'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_cookie(cookie_file):\n",
    "    cookie = cookielib.MozillaCookieJar(cookie_file)\n",
    "    opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(cookie))\n",
    "    response = opener.open('http://archiveofourown.org/works/5051548?view_adult=true')\n",
    "    cookie.save(ignore_discard=True, ignore_expires=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_cookie(cookie_file):\n",
    "    cookie = cookielib.MozillaCookieJar()\n",
    "    cookie.load(cookie_file, ignore_discard=True, ignore_expires=True)\n",
    "    return cookie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save_cookie(cookie_file)\n",
    "cookie = load_cookie(cookie_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_page(base_url, page_number):\n",
    "    #go to any page number.\n",
    "    return base_url+'?page=' +str(page_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def find_works(page):\n",
    "    #Find all works from a works list page.\n",
    "    works_page = bs(urllib2.urlopen(page))\n",
    "    links = []\n",
    "    for link in works_page.find_all('a'):\n",
    "        try:\n",
    "            url = link.get('href')\n",
    "            url_s = [i for i in url.split('/') if i != '']\n",
    "            if 'work' in url and len(url_s) == 2 and str(url_s[1]).isdigit():\n",
    "                    links.append('http://archiveofourown.org'+link.get('href'))\n",
    "        except:\n",
    "            pass\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def show_full_contents(url):\n",
    "    #go through adult contents filtering.\n",
    "    base = bs(urllib2.urlopen(url))\n",
    "    full_url = url\n",
    "    for link in base.find_all('a'):\n",
    "        if 'Proceed' in link.text:\n",
    "            full_url = url +'?view_adult=true'\n",
    "    return full_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_next_chapter(url):\n",
    "    #for multi chapter works, get next chapter link.\n",
    "    req = urllib2.Request(url)\n",
    "    opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(cookie))\n",
    "    page = bs(opener.open(req))\n",
    "    next_chapter = ''\n",
    "    for link in page.find_all('a'):\n",
    "        if 'Next Chapter' in link.text:\n",
    "            next_chapter = ('http://archiveofourown.org' + link.get('href'))\n",
    "    return next_chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_download_link(url):\n",
    "    #may use the download page alternatively, where the html page is cleaner. But this misses some features, so not use\n",
    "    #at the moment.\n",
    "    req = urllib2.Request(url)\n",
    "    opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(cookie))\n",
    "    page = bs(opener.open(req))\n",
    "    for link in page.find_all('a'):\n",
    "        if 'HTML' in link.text:\n",
    "            download_link = 'http://archiveofourown.org' + link.get('href')\n",
    "    return download_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_contents(url):\n",
    "#     get work metadata and contents from the work page.\n",
    "#     print 'Reading url:', url\n",
    "    try:\n",
    "        req = urllib2.Request(url)\n",
    "        opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(cookie))\n",
    "        page = bs(opener.open(req))\n",
    "        contents = str(page.body.text.encode('utf-8')).replace('\\n','A')\n",
    "    except:\n",
    "        print 'Unable to read from this url'\n",
    "        contents = ''\n",
    "        with open('/Users/jingy/Desktop/problematic_url.csv', 'a') as g:\n",
    "            g.write(url)\n",
    "    return url, contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def write_header(outfile):\n",
    "    f = open(outfile, 'a')\n",
    "    writer = csv.writer(f, delimiter=',')\n",
    "    keys = ['Additional_Tags', 'Archive_Warnings', 'Author', 'Bookmarks', 'Category', 'Chapters', 'Characters',\\\n",
    "             'Comments', 'CompleteDate', 'Fandoms', 'Hits', 'Kudos', 'Language', 'Notes', 'PublishDate', 'Rating',\\\n",
    "             'Relationship', 'Summary', 'Text', 'Title', 'Words']\n",
    "    writer.writerow(keys)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def write_work_content(work_dict,outfile):\n",
    "    #write work metadata and contents as values of a sorted dictionary.\n",
    "    f = open(outfile, 'a')\n",
    "    writer = csv.writer(f, delimiter=',')\n",
    "    try:\n",
    "        writer.writerow(OrderedDict(sorted(work_dict.items())).values())\n",
    "    except:\n",
    "        pass\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#creates dictionary for information in a single work.\n",
    "def create_work_dict(url, contents):\n",
    "#     get work metadata and contents into a dictionary.\n",
    "\n",
    "#     print 'Getting work information from:', url\n",
    "    try:\n",
    "        work = {}\n",
    "\n",
    "        rating = re.findall('Rating:(.*?)<br />',contents) \n",
    "        warning = re.findall('Warnings:(.*?)<br />',contents)\n",
    "        fandom = re.findall('Fandoms:A          AAA(.*?)AAAA|Fandom:A          AAA(.*?)AAAA',contents) \n",
    "        category = re.findall('Category:A          AAA(.*?)AAAA',contents)\n",
    "        relationship = re.findall('Relationships:(.*?)<br />',contents)\n",
    "        characters = re.findall('Characters:(.*?)<br />',contents)\n",
    "        additional = re.findall('Additional Tags:(.*?)<br />',contents) \n",
    "        language = re.findall('Language:A      AA(.*?)A      A',contents)\n",
    "        author = re.findall('A    AA(.*?)AAA',contents)\n",
    "        text = re.findall('Work Text:(.*?)AAAAAAAA|Chapter TextA(.*?)AAAAA|Chapter TextAAAAAA(.*?)',contents)\n",
    "        text = [i for i in text[0] if i != ''] if text != [] else []\n",
    "        title = re.findall('AAAAAAAA      (.*?)A    AA',contents)\n",
    "        summary = re.findall('>Summary: <p>(.*?)</p>',contents)\n",
    "        notes = re.findall('Notes:AA(.*?)AA',contents)\n",
    "        publishdate = re.findall('Published:([0-9]*-[0-9]*-[0-9]*)',contents)\n",
    "        completedate = re.findall('Completed:([0-9]*-[0-9]*-[0-9]*)',contents)\n",
    "        words = re.findall('Words:([0-9]*)',contents)\n",
    "        chapters = re.findall('Chapters:([0-9]*/[0-9]*)',contents)\n",
    "        comments = re.findall('Comments:([0-9]*)',contents)\n",
    "        kudos = re.findall('Kudos:([0-9]*)',contents)\n",
    "        bookmarks = re.findall('Bookmarks:([0-9]*)',contents)\n",
    "        hits = re.findall('Hits:([0-9]*)',contents)       \n",
    "        \n",
    "        work['Rating'] = rating[0] if rating != [] else ''\n",
    "        work['Archive_Warnings'] = warning[0] if warning != [] else ''\n",
    "        work['Fandoms'] = [i for i in fandom[0] if i != ''][0] if fandom != [] else ''\n",
    "        work['Category'] = category[0] if category != [] else ''\n",
    "        work['Relationship'] = relationship[0] if relationship != [] else '' \n",
    "        work['Characters'] = characters[0] if characters != [] else ''\n",
    "        work['Additional_Tags'] = additional[0] if additional != [] else ''\n",
    "        work['Language'] = language[0] if language != [] else ''\n",
    "        work['Author'] = author[0] if author != [] else ''\n",
    "        work['Text']= text[0] if text != [] else ''\n",
    "        work['Title']  = title[0] if title != [] else ''\n",
    "        work['Summary'] = summary[0] if summary != [] else ''\n",
    "        work['Notes'] = notes[0] if notes != [] else ''\n",
    "        work['Comments'] = comments[0] if comments != [] else ''\n",
    "        work['PublishDate'] = publishdate[0] if publishdate != [] else ''\n",
    "        work['CompleteDate'] = completedate[0] if completedate != [] else ''\n",
    "        work['Words'] = words[0] if words != [] else ''\n",
    "        work['Chapters'] = chapters[0] if chapters != [] else ''\n",
    "        work['Comments'] = comments[0] if comments != [] else ''\n",
    "        work['Kudos'] = kudos[0] if kudos != [] else ''\n",
    "        work['Bookmarks'] = bookmarks[0] if bookmarks != [] else ''\n",
    "        work['Hits'] = hits[0] if hits != [] else ''\n",
    "    \n",
    "#     print 'Finished with:', url\n",
    "    except:\n",
    "        print 'Something went wrong.'\n",
    "    return work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_single_work(url):\n",
    "    url_full = show_full_contents(url)\n",
    "    u, c = get_contents(url_full)\n",
    "    work = create_work_dict(u, c)\n",
    "    write_work_content(work,outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_chapters(url):\n",
    "    url_full = show_full_contents(url)\n",
    "    chapters_list = []\n",
    "    next_url = get_next_chapter(url_full)\n",
    "    while next_url != '':\n",
    "        chapters_list.append(next_url)\n",
    "        next_url = get_next_chapter(next_url)\n",
    "    return chapters_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get_next_chapter('http://archiveofourown.org/works/3078407?view_adult=true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# s = get_contents('http://archiveofourown.org/works/5051548?view_adult=true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# d = create_work_dict('u',str(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# d = create_work_dict('http://archiveofourown.org/works/5205566',str(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# c = bs(urllib2.urlopen('http://archiveofourown.org/works/3078407/chapters/6678515')).body.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get_chapters('http://archiveofourown.org/works/3078407?view_adult=true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crawling page: 117\n",
      "crawling page: 118\n",
      "crawling page: 119\n",
      "crawling page: 120\n",
      "crawling page: 121\n",
      "crawling page: 122\n",
      "crawling page: 123\n",
      "crawling page: 124\n",
      "crawling page: 125\n",
      "crawling page: 126\n",
      "crawling page: 127\n",
      "crawling page: 128\n",
      "crawling page: 129\n",
      "crawling page: 130\n",
      "crawling page: 131\n",
      "crawling page: 132\n",
      "crawling page: 133\n",
      "crawling page: 134\n",
      "crawling page: 135\n",
      "crawling page: 136\n",
      "crawling page: 137\n",
      "crawling page: 138\n",
      "crawling page: 139\n",
      "crawling page: 140\n",
      "crawling page: 141\n",
      "crawling page: 142\n",
      "crawling page: 143\n",
      "crawling page: 144\n",
      "crawling page: 145\n",
      "crawling page: 146\n",
      "crawling page: 147\n",
      "crawling page: 148\n",
      "crawling page: 149\n",
      "crawling page: 150\n",
      "crawling page: 151\n",
      "Unable to read from this url\n",
      "crawling page: 152\n",
      "crawling page: 153\n",
      "crawling page: 154\n",
      "crawling page: 155\n",
      "crawling page: 156\n",
      "crawling page: 157\n",
      "crawling page: 158\n",
      "Saved 834 works from 158 pages of tag Sherlock Holmes in 210.778252 seconds .\n"
     ]
    }
   ],
   "source": [
    "#main loop\n",
    "write_header(outfile)\n",
    "start_time = time.clock()\n",
    "count = 0\n",
    "\n",
    "for i in range(1001, max_page+1):\n",
    "    try:\n",
    "        page = find_page(start, i)\n",
    "        worklist = find_works(page)\n",
    "        for w in worklist:\n",
    "            ch_list = get_chapters(w)\n",
    "            if ch_list != []:\n",
    "                read_single_work(w)\n",
    "                for ch in ch_list:\n",
    "                    read_single_work(ch)\n",
    "            else:\n",
    "                read_single_work(w)\n",
    "            count += 1\n",
    "\n",
    "        print 'crawling page:', i\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print 'Saved %s works from %s pages of tag %s in %s seconds .' %(count, i, 'Sherlock Holmes', str(time.clock() - start_time))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
